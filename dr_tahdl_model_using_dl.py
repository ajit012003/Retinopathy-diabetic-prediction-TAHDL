# -*- coding: utf-8 -*-
"""DR_TAHDL_model_using-DL(final-year-project).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKKph30F_h7wptXwWPZTQWfG8-Zld7-8
"""



# ====== Cell 0 (final, improved) ======
# Mount Drive, set paths, seeds, robust sanity checks, save enriched run_config
# Colab-ready. Run this first.

import os, json, time, random
from datetime import datetime
import numpy as np
import torch
import pandas as pd
#from utils_hpc import show_or_save, make_grad_scaler



# Colab drive mount
#from google.colab import drive
#drive.mount('/content/drive', force_remount=False)

# ----- Paths (edit if needed) -----
PROJECT_DIR   = r"/home/mazaveri/hpc-prog/ajitkumar/ajit/"
DATASET_DIR   = os.path.join(PROJECT_DIR, "Datasets")

TRAIN_IMG_DIR = os.path.join(DATASET_DIR, "train_images_512", "train_images_512")
TEST_IMG_DIR  = os.path.join(DATASET_DIR, "test_images_512", "test_images_512")

TRAIN_CSV     = os.path.join(DATASET_DIR, "train.csv", "train.csv")
TEST_CSV      = os.path.join(DATASET_DIR, "test.csv", "test.csv")

MODEL_DIR     = os.path.join(PROJECT_DIR, "backend", "models", "result2")
RESULTS_DIR   = os.path.join(PROJECT_DIR, "results", "result2")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

print("Paths:")
print("  PROJECT_DIR :", PROJECT_DIR)
print("  DATASET_DIR :", DATASET_DIR)
print("  TRAIN_IMG_DIR:", TRAIN_IMG_DIR)
print("  TRAIN_CSV   :", TRAIN_CSV)
print("  MODEL_DIR   :", MODEL_DIR)
print("  RESULTS_DIR :", RESULTS_DIR)
print()


# ----- Device + seeds -----
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)
if torch.cuda.is_available():
    try:
        print("  CUDA:", torch.cuda.get_device_name(0))
    except Exception:
        pass
else:
    print("⚠️ WARNING: CUDA not available — training on CPU will be very slow. Use college GPU for full runs.")

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    # deterministic flags for reproducibility (may impact perf)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
print(f"Seeds set to {SEED}\n")

# ----- helper: simple count_images (case-insensitive) -----
def count_images(folder):
    exts = {'.png','.jpg','.jpeg','.tif','.tiff','.bmp'}
    c = 0
    if not os.path.exists(folder):
        return 0
    for root,_,files in os.walk(folder):
        for f in files:
            if os.path.splitext(f.lower())[1] in exts:
                c += 1
    return c

# Quick simple counts
train_img_count_quick = count_images(TRAIN_IMG_DIR)
test_img_count_quick  = count_images(TEST_IMG_DIR)
print("Quick image counts:")
print("  Train images in folder:", train_img_count_quick)
print("  Test  images in folder:", test_img_count_quick)
print()

# ----- robust helper: _count_image_files (recursive, helpful) -----
def _count_image_files(folder, recursive=True):
    exts = {'.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp'}
    total = 0
    if not os.path.exists(folder):
        return 0
    for root, _, files in os.walk(folder):
        for f in files:
            if os.path.splitext(f.lower())[1] in exts:
                total += 1
        if not recursive:
            break
    return total

# ----- CSV exists? -----
if not os.path.exists(TRAIN_CSV):
    raise FileNotFoundError(f"train.csv not found at: {TRAIN_CSV}\nPlease upload train.csv to that path in your Drive (see screenshot).")

df = pd.read_csv(TRAIN_CSV)
print(f"✅ Loaded train.csv — rows: {len(df)}")
print("Columns:", list(df.columns))
print()

# ----- find label column (robust) -----
LABEL_COL = None
for c in df.columns:
    if c.lower() in ('diagnosis','label','target','class'):
        LABEL_COL = c
        break
if LABEL_COL is None:
    # try numeric columns or last column as fallback
    cand = [c for c in df.columns if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c])]
    if cand:
        LABEL_COL = cand[0]
    else:
        LABEL_COL = df.columns[-1]
print(f"Using label column: '{LABEL_COL}' (values shown below)")
try:
    print(df[LABEL_COL].value_counts().sort_index())
except Exception:
    print("  (Could not display label counts; ensure label column exists and is numeric/categorical.)")
print()

# ----- image counts and diagnostics -----
train_count = _count_image_files(TRAIN_IMG_DIR)
test_count  = _count_image_files(TEST_IMG_DIR)
print(f"Found {train_count} image files in TRAIN_IMG_DIR")
print(f"Found {test_count} image files in TEST_IMG_DIR (if any)\n")

if train_count == 0:
    print(f"No images detected in TRAIN_IMG_DIR: {TRAIN_IMG_DIR}")
    if os.path.exists(TRAIN_IMG_DIR):
        subfolders = [d for d in os.listdir(TRAIN_IMG_DIR) if os.path.isdir(os.path.join(TRAIN_IMG_DIR, d))]
        if subfolders:
            print("  • Found subfolders inside train_images (showing up to 10):", subfolders[:10])
            print("    → If your images are inside these, update TRAIN_IMG_DIR or move images up one level.")
        else:
            print("  • No subfolders. Possible causes: images not uploaded, or extensions not standard.")
    else:
        print("  • TRAIN_IMG_DIR path does not exist. Please create folder and upload images.")

    # quick scan under DATASET_DIR to suggest candidate folders
    guessed = []
    for root, _, files in os.walk(DATASET_DIR):
        for f in files:
            if os.path.splitext(f.lower())[1] in ('.png','.jpg','.jpeg','.tif','.tiff','.bmp'):
                guessed.append(os.path.join(root, f))
        if len(guessed) > 20:
            break
    if guessed:
        print(f"  • Quick scan found {len(guessed)} image files under DATASET_DIR (sample):")
        for p in guessed[:5]:
            print("    ", p)
        print("  → If these are train images, set TRAIN_IMG_DIR to their parent folder.")
    raise FileNotFoundError("No images detected in TRAIN_IMG_DIR. Fix path or upload images and re-run.")

# ----- verify filename column in CSV (common names) -----
possible_fname_cols = [c for c in df.columns if c.lower() in ('image','filename','file','id','image_name','id_code','image_id','img_name')]
fname_col = possible_fname_cols[0] if possible_fname_cols else None
if fname_col:
    print("Detected filename column in CSV:", fname_col)
    # sample-check (first 20)
    sampled = df[fname_col].astype(str).iloc[:20].tolist()
    missing = []
    found = 0
    common_exts = ['.png','.jpg','.jpeg','.tif']
    for name in sampled:
        # try exact
        p_exact = os.path.join(TRAIN_IMG_DIR, name)
        if os.path.exists(p_exact):
            found += 1; continue
        # try with extensions appended
        matched=False
        for e in common_exts:
            if os.path.exists(os.path.join(TRAIN_IMG_DIR, name + e)):
                matched=True; found += 1; break
        if not matched:
            # try lowercase variants
            for e in common_exts:
                p = os.path.join(TRAIN_IMG_DIR, name.lower() + e)
                if os.path.exists(p):
                    matched=True; found += 1; break
        if not matched:
            missing.append(name)
    print(f"  Matches in sample: {found}/20. Missing sample filenames (up to 10):", missing[:10])
    if found < 10:
        print("  ⚠️ Many sample filenames not found. Maybe CSV 'id' needs extension, or images are in subfolders.")
else:
    print("Could not auto-detect filename column in CSV. Ensure CSV has a filename/id column matching images in train_images/")

# ----- class order & simple checks -----
try:
    class_order = sorted(list(map(int, df[LABEL_COL].unique())))
except Exception:
    # fallback: string labels
    class_order = sorted(list(df[LABEL_COL].unique()))
print("Detected class labels / order:", class_order)

# ----- Save enriched run_config.json -----
ts = datetime.now().strftime("%Y%m%d-%H%M%S")
run_config = {
    "project_dir": PROJECT_DIR,
    "dataset_dir": DATASET_DIR,
    "train_img_dir": TRAIN_IMG_DIR,
    "test_img_dir": TEST_IMG_DIR,
    "train_csv": TRAIN_CSV,
    "test_csv": TEST_CSV,
    "model_dir": MODEL_DIR,
    "results_dir": RESULTS_DIR,
    "device": str(DEVICE),
    "seed": SEED,
    "img_size": 224,
    "batch_size": 32,
    "epochs": 50,
    "lr": 0.001,
    "use_class_weights": True,
    "timestamp": ts,
    "fname_col": fname_col,
    "label_col": LABEL_COL,
    "class_order": class_order,
    "train_img_count": int(train_count),
    "test_img_count": int(test_count),
    "note": "Cell0 final: enriched run_config with file/label detection and helpful diagnostics."
}
cfg_path = os.path.join(RESULTS_DIR, f"run_config_result2_v1_{ts}.json")
with open(cfg_path, 'w') as f:
    json.dump(run_config, f, indent=2)
print("\nSaved run_config to:", cfg_path)
print("\n ============Cell 0 complete ============")

# ===== Cell 1 (final) =====
# Preprocessing, transforms, dataset class, save a few preprocessed samples
# Assumes Cell0 created: RESULTS_DIR, TRAIN_IMG_DIR, df, LABEL_COL

import os, sys, traceback
from pathlib import Path
from PIL import Image
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T
import torchvision.utils as vutils
import matplotlib.pyplot as plt

# --------------------
# User-configurable flags
# --------------------
IMG_SIZE = 224
BATCH_SIZE = 32
NUM_WORKERS = 0
APPLY_CLAHE = True           # set False to skip CLAHE
USE_ALBUMENTATIONS = True    # try Albumentations, fallback to torchvision if missing
SAVE_SAMPLES_DIR = os.path.join(RESULTS_DIR, "preprocessed_samples_result2")
os.makedirs(SAVE_SAMPLES_DIR, exist_ok=True)

# --------------------
# CLAHE helper (LAB L-channel) - input PIL.Image -> output PIL.Image
# --------------------
def apply_clahe_rgb_pil(img_pil, clipLimit=2.0, tileGridSize=(8,8)):
    """
    Apply CLAHE on L channel (LAB) and return PIL.Image (RGB uint8).
    """
    try:
        img = np.asarray(img_pil)  # HWC RGB
        bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
        cl = clahe.apply(l)
        lab_cl = cv2.merge((cl, a, b))
        bgr_cl = cv2.cvtColor(lab_cl, cv2.COLOR_LAB2BGR)
        rgb_cl = cv2.cvtColor(bgr_cl, cv2.COLOR_BGR2RGB)
        return Image.fromarray(rgb_cl)
    except Exception:
        # If something fails, return original
        return img_pil

# --------------------
# Optional simple center circular crop (fundus)
# --------------------
def circular_crop_pil(img_pil, padding=0):
    w, h = img_pil.size
    cx, cy = w//2, h//2
    r = min(cx, cy) - padding
    arr = np.array(img_pil.convert("RGB"))
    yy, xx = np.ogrid[:h, :w]
    mask = (xx - cx)**2 + (yy - cy)**2 <= r*r
    out = np.zeros_like(arr)
    out[mask] = arr[mask]
    return Image.fromarray(out)

# --------------------
# Try to import albumentations
# --------------------
A = None
ToTensorV2 = None
if USE_ALBUMENTATIONS:
    try:
        import albumentations as A
        try:
            from albumentations.pytorch import ToTensorV2
        except Exception:
            ToTensorV2 = None
        print("Albumentations available. Using albumentations pipeline.")
    except Exception:
        A = None
        ToTensorV2 = None
        print("Albumentations not available — falling back to torchvision transforms.")
        # keep USE_ALBUMENTATIONS False for downstream logic
        USE_ALBUMENTATIONS = False

# --------------------
# Define transforms (albumentations if possible, else torchvision)
# Note: CLAHE is applied by Dataset before albumentations/torch transforms if APPLY_CLAHE True
# --------------------
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

if USE_ALBUMENTATIONS and A is not None:
    # Albumentations pipeline (expects numpy HWC uint8)
    train_transform = A.Compose([
        A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.2),
        A.Rotate(limit=20, p=0.4),
        A.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, p=0.4),
        A.RandomBrightnessContrast(p=0.3),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ToTensorV2() if ToTensorV2 is not None else A.Lambda(name="to_tensor_fallback", image=lambda x, **kwargs: x)
    ], p=1.0)

    val_transform = A.Compose([
        A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ToTensorV2() if ToTensorV2 is not None else A.Lambda(name="to_tensor_fallback", image=lambda x, **kwargs: x)
    ], p=1.0)
else:
    # torchvision transforms fallback (PIL -> Tensor)
    train_transform = T.Compose([
        T.RandomHorizontalFlip(p=0.5),
        T.RandomVerticalFlip(p=0.2),
        T.RandomRotation(20),
        T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),
        T.Resize((IMG_SIZE, IMG_SIZE)),
        T.ToTensor(),
        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
    ])

    val_transform = T.Compose([
        T.Resize((IMG_SIZE, IMG_SIZE)),
        T.ToTensor(),
        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
    ])

# --------------------
# FundusDataset: supports albumentations (numpy HWC) and torchvision (PIL)
# returns: (img_tensor, label, filename)
# --------------------
class FundusDataset(Dataset):
    def __init__(self, df, images_dir, fname_col=None, label_col=None,
                 apply_clahe=True, apply_circular_crop=True, transform=None, common_exts=('.png','.jpg','.jpeg','.tif','.tiff')):
        self.df = df.reset_index(drop=True)
        self.images_dir = images_dir
        # detect filename column if not provided
        if fname_col is None:
            candidates = [c for c in df.columns if c.lower() in ('image','filename','file','id','image_name','id_code','image_id','img_name')]
            self.fname_col = candidates[0] if candidates else None
        else:
            self.fname_col = fname_col
        self.label_col = label_col if label_col in df.columns else None
        self.apply_clahe = apply_clahe
        self.apply_circular_crop = apply_circular_crop
        self.transform = transform
        self.common_exts = common_exts

        if self.fname_col is None:
            raise KeyError("Could not detect filename column in df. Provide fname_col explicitly when constructing FundusDataset.")

    def __len__(self):
        return len(self.df)

    def _resolve_path(self, name):
        # try exact
        p = os.path.join(self.images_dir, name)
        if os.path.exists(p):
            return p
        # try append exts
        for e in self.common_exts:
            p2 = os.path.join(self.images_dir, str(name) + e)
            if os.path.exists(p2):
                return p2
        # try lowercase + ext
        for e in self.common_exts:
            p3 = os.path.join(self.images_dir, str(name).lower() + e)
            if os.path.exists(p3):
                return p3
        # last resort: substring match (may be slow)
        for f in os.listdir(self.images_dir):
            if str(name) in f:
                return os.path.join(self.images_dir, f)
        return None

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        fname = str(row[self.fname_col])
        path = self._resolve_path(fname)
        if path is None:
            raise FileNotFoundError(f"Image for '{fname}' not found in {self.images_dir}")
        # load PIL
        img_pil = Image.open(path).convert("RGB")
        # CLAHE and circular crop (PIL)
        if self.apply_clahe:
            try:
                img_pil = apply_clahe_rgb_pil(img_pil)
            except Exception:
                # fallback keep original
                pass
        if self.apply_circular_crop:
            try:
                img_pil = circular_crop_pil(img_pil, padding=10)
            except Exception:
                pass

        # Apply transform. If albumentations (compose) expects numpy HWC uint8
        if A is not None and isinstance(self.transform, A.core.composition.Compose):
            img_np = np.array(img_pil)  # HWC RGB (uint8)
            augmented = self.transform(image=img_np)
            img_t = augmented['image']
            # If ToTensorV2 not available we used Lambda fallback -> img_t still numpy
            if isinstance(img_t, np.ndarray):
                # convert to torch tensor HWC->CHW float [0,1] normalized manually
                img_t = torch.from_numpy(img_t.astype(np.float32).transpose(2,0,1) / 255.0)
                # normalize using IMAGENET mean/std
                mean = torch.tensor(IMAGENET_MEAN).view(3,1,1)
                std  = torch.tensor(IMAGENET_STD).view(3,1,1)
                img_t = (img_t - mean) / std
        else:
            # torchvision pipeline expects PIL images
            if self.transform is not None:
                img_t = self.transform(img_pil)
            else:
                img_t = T.ToTensor()(img_pil)

        label = None
        if self.label_col is not None:
            label = int(row[self.label_col])
        return img_t, label, os.path.basename(path)

# --------------------
# Create dataset objects and a small loader for sanity-check
# --------------------
# use df from Cell0
fname_col = run_config.get("fname_col", None) if 'run_config' in globals() else None
label_col = run_config.get("label_col", LABEL_COL) if 'run_config' in globals() else LABEL_COL

# create dataset objects - use full df but you'll usually split later
dataset_full = FundusDataset(df=df, images_dir=TRAIN_IMG_DIR, fname_col=fname_col, label_col=label_col,
                             apply_clahe=True, apply_circular_crop=True, transform=train_transform)

# quick DataLoader smoke test (small batch)
dl = DataLoader(dataset_full, batch_size=min(4, BATCH_SIZE), shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)

# fetch one batch and save examples
try:
    batch = next(iter(dl))
    imgs, labels, fnames = batch
    # imgs is tensor. Save first up to 4 samples (unnormalize for visual)
    def unnormalize(tensor_img):
        img = tensor_img.cpu().clone()
        for c in range(3):
            img[c] = img[c] * IMAGENET_STD[c] + IMAGENET_MEAN[c]
        img = torch.clamp(img, 0, 1)
        return T.ToPILImage()(img)

    for i in range(min(4, imgs.size(0))):
        pil = unnormalize(imgs[i])
        savep = os.path.join(SAVE_SAMPLES_DIR, f"sample_{i}_{fnames[i]}")
        pil.save(savep)
        print("Saved preprocessed sample:", savep)
except Exception as e:
    print("Warning: could not sample/save preprocessed images — traceback below")
    traceback.print_exc()

print("\n ============ Cell 1 complete — transforms & dataset ready.============")




# ===== Cell 2 (final) =====
# Enhanced Dataset + Sequence-mode + collate_fn + dataloader factory
# Run after Cell0 & Cell1 (they provide: df, TRAIN_IMG_DIR, RESULTS_DIR, train_transform, val_transform)

import os, json, math, random, itertools
from collections import defaultdict, Counter
from typing import List, Tuple, Optional
from PIL import Image
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler

# ---------- Config ----------
IMG_SIZE = 224  # keep consistent with Cell1
NUM_WORKERS = 0
DEFAULT_COMMON_EXTS = ('.png','.jpg','.jpeg','.tif','.tiff')
CACHE_IN_MEMORY = False   # set True on small debug subset
SEQUENCE_MODE = False     # set True to use patient sequences (change below or via constructor)
PAD_VALUE = 0.0           # padding value for images

# ---------- Utility: safe image load + caching ----------
class ImageResolver:
    def __init__(self, images_dir, common_exts=DEFAULT_COMMON_EXTS):
        self.images_dir = images_dir
        self.common_exts = tuple(common_exts)
        self._cache = {}   # name -> fullpath (or None if not found)

    def resolve(self, name: str) -> Optional[str]:
        if name in self._cache:
            return self._cache[name]
        # direct
        p = os.path.join(self.images_dir, name)
        if os.path.exists(p):
            self._cache[name] = p; return p
        # append exts
        for e in self.common_exts:
            p2 = os.path.join(self.images_dir, str(name) + e)
            if os.path.exists(p2):
                self._cache[name] = p2; return p2
        # lowecase variants
        nlow = str(name).lower()
        for e in self.common_exts:
            p3 = os.path.join(self.images_dir, nlow + e)
            if os.path.exists(p3):
                self._cache[name] = p3; return p3
        # substring fallback (last resort)
        for f in os.listdir(self.images_dir):
            if str(name) in f:
                p4 = os.path.join(self.images_dir, f)
                self._cache[name] = p4; return p4
        self._cache[name] = None
        return None

# ---------- Enhanced FundusDataset ----------
class EnhancedFundusDataset(Dataset):
    """
    Modes:
      - sequence_mode=False: returns (img_tensor, label, filename)
      - sequence_mode=True: expects patient_col in df -> builds samples per patient and returns
         (seq_tensor, seq_len, label, list_of_filenames)
         seq_tensor shape: (T, C, H, W) padded to max T in batch by collate_fn
    """
    def __init__(self, df, images_dir, fname_col=None, label_col=None, patient_col=None,
                 transform=None, apply_clahe=True, apply_circular_crop=True,
                 cache_in_memory=False, sequence_mode=False, common_exts=DEFAULT_COMMON_EXTS):
        self.df = df.reset_index(drop=True)
        self.images_dir = images_dir
        self.transform = transform
        self.apply_clahe = apply_clahe
        self.apply_circular_crop = apply_circular_crop
        self.cache_in_memory = cache_in_memory
        self.sequence_mode = sequence_mode
        self.common_exts = common_exts

        # detect filename col if not provided
        if fname_col is None:
            candidates = [c for c in self.df.columns if c.lower() in ('image','filename','file','id','image_name','id_code','image_id','img_name')]
            self.fname_col = candidates[0] if candidates else None
        else:
            self.fname_col = fname_col
        if self.fname_col is None:
            raise KeyError("Filename column not detected. Provide fname_col explicitly.")

        # label col
        self.label_col = label_col if (label_col in self.df.columns) else None

        # patient column for sequence-mode (optional)
        self.patient_col = patient_col if (patient_col in self.df.columns) else None
        if sequence_mode and self.patient_col is None:
            raise KeyError("sequence_mode=True but no patient_col provided/found in df.")

        # resolver + memory cache
        self.resolver = ImageResolver(images_dir, common_exts=self.common_exts)
        self._memcache = {}   # path -> tensor (if cache_in_memory True)

        # build index for sequence mode or simple index
        if self.sequence_mode:
            # Group rows by patient_id, collect list of filenames and labels
            self.patient_groups = {}   # patient_id -> {'filenames':[...], 'labels':[...]}
            grouped = defaultdict(list)
            for _, r in self.df.iterrows():
                pid = r[self.patient_col]
                grouped[pid].append(r)
            # preserve order as given (if temporal ordering required, provide timestamp col and sort)
            for pid, rows in grouped.items():
                filenames = [str(r[self.fname_col]) for r in rows]
                labels = [int(r[self.label_col]) if self.label_col is not None else None for r in rows]
                self.patient_groups[pid] = {'filenames': filenames, 'labels': labels}
            self.patient_ids = list(self.patient_groups.keys())
        else:
            # simple list of rows
            self.index_map = list(range(len(self.df)))

    def __len__(self):
        return len(self.patient_ids) if self.sequence_mode else len(self.index_map)

    def _load_img_tensor(self, path: str):
        # caching
        if self.cache_in_memory and path in self._memcache:
            return self._memcache[path]
        # load PIL -> apply CLAHE/crop if needed -> transform
        pil = Image.open(path).convert("RGB")
        # apply CLAHE / circular crop if needed
        # Note: Cell1 provided apply_clahe_rgb_pil and circular_crop_pil; call them if available
        if self.apply_clahe and 'apply_clahe_rgb_pil' in globals():
            try:
                pil = apply_clahe_rgb_pil(pil)
            except Exception:
                pass
        if self.apply_circular_crop and 'circular_crop_pil' in globals():
            try:
                pil = circular_crop_pil(pil, padding=10)
            except Exception:
                pass
        # apply transform pipeline (torchvision or albumentations)
        if 'A' in globals() and A is not None and isinstance(self.transform, A.core.composition.Compose):
            img_np = np.array(pil)
            aug = self.transform(image=img_np)
            img_t = aug['image']
            # if ToTensorV2 used, img_t is torch tensor; else numpy
            if isinstance(img_t, np.ndarray):
                img_t = torch.from_numpy(img_t.astype(np.float32).transpose(2,0,1) / 255.0)
                mean = torch.tensor([0.485,0.456,0.406]).view(3,1,1)
                std  = torch.tensor([0.229,0.224,0.225]).view(3,1,1)
                img_t = (img_t - mean) / std
        else:
            # torchvision transforms expect PIL
            if self.transform is not None:
                img_t = self.transform(pil)
            else:
                img_t = T.ToTensor()(pil)

        if self.cache_in_memory:
            self._memcache[path] = img_t
        return img_t

    def __getitem__(self, idx):
        if self.sequence_mode:
            pid = self.patient_ids[idx]
            entry = self.patient_groups[pid]
            fnames = entry['filenames']
            labels = entry['labels']  # list (same length)
            tensors = []
            real_fnames = []
            for nm in fnames:
                p = self.resolver.resolve(nm)
                if p is None:
                    raise FileNotFoundError(f"File '{nm}' for patient '{pid}' not found in {self.images_dir}")
                img_t = self._load_img_tensor(p)
                tensors.append(img_t)
                real_fnames.append(os.path.basename(p))
            # determine sequence label: majority vote of labels (if labels present), else None
            seq_label = None
            if any(l is not None for l in labels):
                lbls = [l for l in labels if l is not None]
                seq_label = int(Counter(lbls).most_common(1)[0][0])
            # return list of tensors (variable length), seq_label, patient id, filenames
            return tensors, seq_label, pid, real_fnames
        else:
            row_idx = self.index_map[idx]
            row = self.df.iloc[row_idx]
            name = str(row[self.fname_col])
            p = self.resolver.resolve(name)
            if p is None:
                raise FileNotFoundError(f"File '{name}' not found in {self.images_dir}")
            img_t = self._load_img_tensor(p)
            lbl = int(row[self.label_col]) if self.label_col is not None else None
            return img_t, lbl, os.path.basename(p)

# ---------- collate_fn for sequence batches ----------
def collate_fn_sequences(batch):
    """
    batch: list of (tensors_list, label, patient_id, fnames_list)
    returns:
      seqs_padded: Tensor (B, T_max, C, H, W)
      lengths: Tensor (B,)
      labels: Tensor (B,) or list of None
      patient_ids: list
      fnames_batch: list of list
    """
    # extract lengths and max len
    lengths = [len(item[0]) for item in batch]
    T_max = max(lengths)
    B = len(batch)
    # assume all tensors same C,H,W
    c,h,w = batch[0][0][0].shape
    seqs = torch.zeros((B, T_max, c, h, w), dtype=batch[0][0][0].dtype)
    labels = []
    patient_ids = []
    fnames_all = []
    for i, (tlist, label, pid, fnames) in enumerate(batch):
        L = len(tlist)
        for j in range(L):
            seqs[i, j] = tlist[j]
        labels.append(label if label is not None else -1)
        patient_ids.append(pid)
        fnames_all.append(fnames)
    lengths_t = torch.tensor(lengths, dtype=torch.long)
    labels_t = torch.tensor(labels, dtype=torch.long)
    return seqs, lengths_t, labels_t, patient_ids, fnames_all

# ---------- collate_fn for single-image batches ----------
def collate_fn_single(batch):
    imgs = torch.stack([item[0] for item in batch], dim=0)
    labels = torch.tensor([item[1] if item[1] is not None else -1 for item in batch], dtype=torch.long)
    fnames = [item[2] for item in batch]
    return imgs, labels, fnames

# ---------- dataloader factory ----------
def make_dataloaders(dataset: EnhancedFundusDataset, batch_size:int=16, num_workers:int=0,
                     use_weighted_sampler:bool=False, shuffle_when_no_sampler:bool=True):
    if use_weighted_sampler and dataset.label_col is not None and not dataset.sequence_mode:
        # compute per-sample weights from dataset.df (for single-image dataset)
        labels = dataset.df[dataset.fname_col].apply(lambda x: None)  # placeholder
        # simpler: use train_df if exists - expect dataset.df to be train split
        try:
            y = dataset.df[dataset.label_col].astype(int).values
            classes, counts = np.unique(y, return_counts=True)
            class_weights = {int(c): 1.0/counts[i] for i,c in enumerate(classes)}
            sample_weights = np.array([class_weights[int(r[dataset.label_col])] for _, r in dataset.df.iterrows()], dtype=float)
            sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)
            shuffle_flag = False
        except Exception as e:
            print("Could not build WeightedRandomSampler:", e)
            sampler = None
            shuffle_flag = shuffle_when_no_sampler
    else:
        sampler = None
        shuffle_flag = shuffle_when_no_sampler

    if dataset.sequence_mode:
        collate = collate_fn_sequences
        if sampler is not None:
            print("Warning: sampler used with sequence_mode may need careful design (sampling patients).")
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=(sampler is None and shuffle_flag),
                            sampler=sampler, num_workers=num_workers, pin_memory=True, collate_fn=collate)
    else:
        collate = collate_fn_single
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=(sampler is None and shuffle_flag),
                            sampler=sampler, num_workers=num_workers, pin_memory=True, collate_fn=collate)
    return loader, sampler

# ---------- Save dataset metadata helper ----------
def save_dataset_meta(dataset: EnhancedFundusDataset, results_dir: str, fname="dataset_meta_result2.json"):
    meta = {
        "images_dir": dataset.images_dir,
        "n_samples": len(dataset),
        "sequence_mode": dataset.sequence_mode,
        "fname_col": dataset.fname_col,
        "label_col": dataset.label_col,
        "patient_col": dataset.patient_col,
        "cache_in_memory": dataset.cache_in_memory,
    }
    path = os.path.join(results_dir, fname)
    with open(path, "w") as f:
        json.dump(meta, f, indent=2)
    print("Saved dataset meta to:", path)

# ---------- Smoke usage example ----------
# Use df loaded in Cell0 and transforms from Cell1
# If you want sequence-mode, set sequence_mode=True and pass patient_col name (exists in your CSV)
sequence_mode = SEQUENCE_MODE  # change if you want
cache_mem = CACHE_IN_MEMORY

# Detect common columns automatically
fname_col_guess = None
possible_fname_cols = [c for c in df.columns if c.lower() in ('image','filename','file','id','image_name','id_code','image_id','img_name')]
if possible_fname_cols:
    fname_col_guess = possible_fname_cols[0]

possible_patient_cols = [c for c in df.columns if c.lower() in ('patient_id','patient','patientcode','patient_code','pid')]
patient_col_guess = possible_patient_cols[0] if possible_patient_cols else None

label_col_guess = None
for c in df.columns:
    if c.lower() in ('diagnosis','label','target','class'):
        label_col_guess = c; break

print("Detected columns -> fname_col:", fname_col_guess, " label_col:", label_col_guess, " patient_col:", patient_col_guess)

# instantiate dataset(s)
if sequence_mode:
    ds = EnhancedFundusDataset(df=df, images_dir=TRAIN_IMG_DIR, fname_col=fname_col_guess, label_col=label_col_guess,
                               patient_col=patient_col_guess, transform=val_transform, cache_in_memory=cache_mem,
                               sequence_mode=True)
else:
    ds = EnhancedFundusDataset(df=df, images_dir=TRAIN_IMG_DIR, fname_col=fname_col_guess, label_col=label_col_guess,
                               transform=val_transform, cache_in_memory=cache_mem, sequence_mode=False)

# make loader
loader, sampler = make_dataloaders(ds, batch_size=8, num_workers=NUM_WORKERS, use_weighted_sampler=True)

print("Dataset length:", len(ds))
print("Loader batches (approx):", math.ceil(len(ds)/8))

# quick sample fetch & print shapes
batch = next(iter(loader))
if sequence_mode:
    seqs, lengths, labels, pids, fnames = batch
    print("Sequence batch shapes -> seqs:", seqs.shape, "lengths:", lengths, "labels:", labels)
else:
    imgs, labels, fnames = batch
    print("Single batch shapes -> imgs:", imgs.shape, "labels:", labels.shape)

# save meta
save_dataset_meta(ds, RESULTS_DIR)

print("============ Cell 2 done. ============")

# ===== Cell 3: Model modules (Multiscale backbone + Attention modules + LSTM block) =====
# Run after Cell0..Cell2. Saves a small model summary file to RESULTS_DIR/result2.
import os, math, time
import torch
import torch.nn as nn
import torch.nn.functional as F
from datetime import datetime

TS = datetime.now().strftime("%Y%m%d-%H%M%S")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------
# Basic conv block
# --------------------
class ConvBlock(nn.Module):
    """Conv -> BN -> ReLU (same padding)"""
    def __init__(self, in_ch, out_ch, k=3, stride=1, padding=None):
        super().__init__()
        if padding is None:
            padding = k // 2
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=stride, padding=padding, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.net(x)

# --------------------
# Multiscale backbone (paper exact)
# --------------------
class MultiscaleBackbone(nn.Module):
    """
    Base convs: (3x3,32) -> (3x3,64) -> MaxPool(2)
    Three branches (3x3,5x5,7x7): 128 -> 256 -> Pool(2)
    Concat -> GlobalAvgPool -> Linear projection to out_dim
    """
    def __init__(self, in_ch=3, base_filters=(32,64), branch_filters=(128,256), out_dim=256, return_intermediate=False):
        super().__init__()
        f1, f2 = base_filters
        bf1, bf2 = branch_filters
        self.return_intermediate = return_intermediate

        # base
        self.conv1 = ConvBlock(in_ch, f1, k=3)
        self.conv2 = ConvBlock(f1, f2, k=3)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # branch 1 (3x3)
        self.b1_1 = ConvBlock(f2, bf1, k=3)
        self.b1_2 = ConvBlock(bf1, bf2, k=3)
        self.b1_pool = nn.MaxPool2d(2,2)

        # branch 2 (5x5)
        self.b2_1 = ConvBlock(f2, bf1, k=5, padding=2)
        self.b2_2 = ConvBlock(bf1, bf2, k=5, padding=2)
        self.b2_pool = nn.MaxPool2d(2,2)

        # branch 3 (7x7)
        self.b3_1 = ConvBlock(f2, bf1, k=7, padding=3)
        self.b3_2 = ConvBlock(bf1, bf2, k=7, padding=3)
        self.b3_pool = nn.MaxPool2d(2,2)

        total_ch = bf2 * 3
        self.global_pool = nn.AdaptiveAvgPool2d((1,1))
        self.proj = nn.Sequential(
            nn.Flatten(),
            nn.Linear(total_ch, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(inplace=True)
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)

    def forward(self, x):
        # x: (B,3,H,W)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.pool1(x)   # downsample

        # branches
        p1 = self.b1_1(x); p1 = self.b1_2(p1); p1 = self.b1_pool(p1)
        p2 = self.b2_1(x); p2 = self.b2_2(p2); p2 = self.b2_pool(p2)
        p3 = self.b3_1(x); p3 = self.b3_2(p3); p3 = self.b3_pool(p3)

        cat = torch.cat([p1, p2, p3], dim=1)   # (B, total_ch, H', W')
        pooled = self.global_pool(cat)         # (B, total_ch, 1,1)
        feat = self.proj(pooled)               # (B, out_dim)

        if self.return_intermediate:
            # also return feature map (for grad-cam, attention visualization)
            return feat, cat
        return feat

# --------------------
# CBAM: ChannelAttention + SpatialAttention
# --------------------
class ChannelAttention(nn.Module):
    def __init__(self, in_ch, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.mlp = nn.Sequential(
            nn.Linear(in_ch, in_ch // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_ch // reduction, in_ch, bias=False)
        )
        self.sig = nn.Sigmoid()
    def forward(self, x):
        b,c,_,_ = x.size()
        avg = self.avg_pool(x).view(b,c)
        maxv = self.max_pool(x).view(b,c)
        out = self.mlp(avg) + self.mlp(maxv)
        out = self.sig(out).view(b,c,1,1)
        return x * out, out  # return scaled feature and att map

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        padding = kernel_size // 2
        self.conv = nn.Conv2d(2,1,kernel_size=kernel_size,padding=padding,bias=False)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        # x: (B,C,H,W)
        avg = torch.mean(x, dim=1, keepdim=True)
        mx,_ = torch.max(x, dim=1, keepdim=True)
        cat = torch.cat([avg, mx], dim=1)  # (B,2,H,W)
        att = self.sig(self.conv(cat))     # (B,1,H,W)
        return x * att, att

class CBAM(nn.Module):
    def __init__(self, in_ch, reduction=16, spatial_kernel=7):
        super().__init__()
        self.ca = ChannelAttention(in_ch, reduction=reduction)
        self.sa = SpatialAttention(kernel_size=spatial_kernel)
    def forward(self, x, return_attentions=False):
        x_ca, ca_map = self.ca(x)
        x_sa, sa_map = self.sa(x_ca)
        if return_attentions:
            return x_sa, {'channel': ca_map, 'spatial': sa_map}
        return x_sa

# --------------------
# Additive (Bahdanau-like) attention for temporal LSTM outputs
# --------------------
class AdditiveAttention(nn.Module):
    """
    Input:
      queries: (B, T, D)  (e.g., LSTM outputs)
      key: optional (not used here)
    Output:
      context: (B, D)  weighted sum over time
      attn_weights: (B, T)
    """
    def __init__(self, dim, hidden_dim=None):
        super().__init__()
        if hidden_dim is None:
            hidden_dim = max(32, dim//2)
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)
        self.v  = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, seq_out, mask=None):
        # seq_out: (B, T, D)
        # compute scores
        # energy = v(tanh(w1(seq_out) + b)) ; simplified additive self-att
        score = self.v(torch.tanh(self.w1(seq_out)))  # (B,T,1)
        score = score.squeeze(-1)                     # (B,T)
        if mask is not None:
            score = score.masked_fill(mask==0, float('-1e9'))
        weights = torch.softmax(score, dim=1)         # (B,T)
        # context: weighted sum of seq_out
        context = torch.sum(seq_out * weights.unsqueeze(-1), dim=1)  # (B, D)
        return context, weights

# --------------------
# LSTM Block (2-layer) wrapper
# --------------------
class LSTMBlock(nn.Module):
    """
    2-layer LSTM wrapper.
    Input: sequence embeddings (B, T, D_in)
    Returns: outputs (B, T, hidden*directions), last_hidden (num_layers*directions, B, hidden)
    """
    def __init__(self, in_dim, hidden_dim=256, num_layers=2, bidirectional=False, dropout=0.5):
        super().__init__()
        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True,
                            bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional

    def forward(self, seq, lengths=None):
        # seq: (B, T, D)
        # If lengths provided, pack
        if lengths is not None:
            packed = nn.utils.rnn.pack_padded_sequence(seq, lengths.cpu(), batch_first=True, enforce_sorted=False)
            packed_out, (h_n, c_n) = self.lstm(packed)
            out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,hidden*dir)
        else:
            out, (h_n, c_n) = self.lstm(seq)
        return out, (h_n, c_n)

# --------------------
# Utility: count params
# --------------------
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# --------------------
# Helper container (modules ready to assemble)
# --------------------
class TAHDLBackboneModules:
    def __init__(self, device=DEVICE, out_feat_dim=1024):
        self.device = device
        self.backbone = MultiscaleBackbone(in_ch=3, base_filters=(32,64), branch_filters=(128,256),
                                           out_dim=out_feat_dim, return_intermediate=True).to(device)
        # CBAM applied on concatenated feature-map channels (bf2*3)
        total_ch = 256 * 3
        self.cbam = CBAM(in_ch=total_ch, reduction=16).to(device)
        # LSTM block for temporal features (in_dim = out_feat_dim)
        self.lstm = LSTMBlock(in_dim=out_feat_dim, hidden_dim=256, num_layers=2, bidirectional=False, dropout=0.5).to(device)
        # Attention (after LSTM)
        self.attention = AdditiveAttention(dim=256)
        # classifier heads will be in Cell4 (FC layers)
    def summary(self):
        s = []
        s.append("TAHDLBackboneModules summary")
        s.append(f"Device: {self.device}")
        s.append(f"Backbone params: {count_parameters(self.backbone)}")
        s.append(f"CBAM params: {count_parameters(self.cbam)}")
        s.append(f"LSTM params: {count_parameters(self.lstm)}")
        s.append(f"AdditiveAttention params: {count_parameters(self.attention)}")
        return "\n".join(s)

# --------------------
# Smoke tests: dummy input and optional loader test
# --------------------
print("DEVICE:", DEVICE)
modules = TAHDLBackboneModules(device=DEVICE, out_feat_dim=1024)
summary_txt = modules.summary()
print(summary_txt)

# Save summary to RESULTS_DIR
try:
    info_path = os.path.join(RESULTS_DIR, f"model_modules_summary_result2_{TS}.txt")
    with open(info_path, "w") as f:
        f.write(summary_txt + "\n")
    print("Saved module summary to:", info_path)
except Exception as e:
    print("Could not save summary:", e)

# Dummy forward test (single image -> backbone -> CBAM -> proj)
try:
    dummy = torch.randn((2,3,IMG_SIZE,IMG_SIZE)).to(DEVICE)
    modules.backbone.eval()
    with torch.no_grad():
        feat, fmap = modules.backbone(dummy)   # feat (B, out_dim), fmap (B, total_ch, H', W')
        fmap_cbam, att_maps = modules.cbam(fmap, return_attentions=True)
    print("Dummy backbone feat shape:", feat.shape)
    print("Feature map shape:", fmap.shape)
    print("CBAM output fmap shape:", fmap_cbam.shape)
    print("CBAM attention shapes -> channel:", att_maps['channel'].shape, " spatial:", att_maps['spatial'].shape)
except Exception as e:
    print("Dummy forward failed:", e)

# If loader available, try one pass to produce per-image embeddings
if 'loader' in globals():
    try:
        batch = next(iter(loader))
        if isinstance(batch, (list,tuple)):
            imgs = batch[0]
        else:
            imgs = batch
        imgs = imgs.to(DEVICE)
        modules.backbone.eval()
        with torch.no_grad():
            feats, fmap = modules.backbone(imgs)
        print("Loader-based smoke: feats shape:", feats.shape)
    except Exception as e:
        print("Loader smoke test failed:", e)


#====== Cell 3: Preprocessing, transforms, Dataset, Dataloaders (with show_by_class) ======
# - Resize, normalization, CLAHE (color normalization)
# - Custom PyTorch Dataset that reads CSV filenames + labels
# - Train/Val split, DataLoaders, class weights saved to results
# - Added show_by_class(...) to display one sample per class (0..4)

import os, json, random
from pathlib import Path
from PIL import Image
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torchvision.transforms as T
from sklearn.model_selection import train_test_split
from collections import Counter
import matplotlib.pyplot as plt

# --- load run config we saved earlier (fallback defaults) ---
cfg_path_candidates = list(Path(RESULTS_DIR).glob("run_config_result2_v1_*.json"))
if len(cfg_path_candidates) > 0:
    cfg_path = str(sorted(cfg_path_candidates)[-1])
    with open(cfg_path, 'r') as f:
        run_cfg = json.load(f)
    print("Loaded run config:", cfg_path)
else:
    # fallback if not found
    run_cfg = {"img_size":224, "batch_size":16, "epochs":50, "use_class_weights":True, "seed":42}
    print("run_config not found in RESULTS_DIR, using fallback values.")

IMG_SIZE = int(run_cfg.get("img_size", 224))
BATCH_SIZE = int(run_cfg.get("batch_size", 16))
RANDOM_STATE = int(run_cfg.get("seed", 42))
NUM_WORKERS = 0  # Colab friendly; increase on your GPU machine

# ------------ helper: CLAHE color normalization ------------
def apply_clahe_rgb(pil_img, clipLimit=2.0, tileGridSize=(8,8)):
    """
    Input: PIL.Image (RGB)
    Output: numpy array (H,W,3) uint8 after CLAHE on L channel (in LAB)
    """
    img = np.array(pil_img)  # RGB
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
    cl = clahe.apply(l)
    lab_cl = cv2.merge((cl,a,b))
    bgr_cl = cv2.cvtColor(lab_cl, cv2.COLOR_LAB2BGR)
    rgb_cl = cv2.cvtColor(bgr_cl, cv2.COLOR_BGR2RGB)
    return rgb_cl

# ------------ optional segmentation placeholder (simple vessel-like enhance) ------------
def simple_segmentation_enhance(rgb_np):
    """
    Placeholder: Enhance contrast and apply morphological ops.
    Replace with your real segmentation if you have masks.
    """
    gray = cv2.cvtColor(rgb_np, cv2.COLOR_RGB2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    g = clahe.apply(gray)
    blur = cv2.GaussianBlur(g, (0,0), sigmaX=3)
    unsharp = cv2.addWeighted(g, 1.5, blur, -0.5, 0)
    fused = np.stack([unsharp, unsharp, unsharp], axis=-1)
    out = cv2.addWeighted(rgb_np, 0.7, fused, 0.3, 0)
    return out

# ------------ torchvision transforms for tensor conversion & augment (simple) ------------
train_transform = T.Compose([
    T.Resize((IMG_SIZE, IMG_SIZE)),
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(degrees=15),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

val_transform = T.Compose([
    T.Resize((IMG_SIZE, IMG_SIZE)),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

# ------------ Custom Dataset ------------
possible_fname_cols = [c for c in df.columns if c.lower() in ('image','id','id_code','filename','image_name','image_id','img_name','file')]
if len(possible_fname_cols) == 0:
    fname_col = df.columns[0]
    print("Warning: couldn't detect filename col. Using first CSV column:", fname_col)
else:
    fname_col = possible_fname_cols[0]
    print("Using filename column:", fname_col)

label_col = None
if any(c.lower()=='diagnosis' for c in df.columns):
    label_col = [c for c in df.columns if c.lower()=='diagnosis'][0]
else:
    for c in df.columns:
        if c.lower() in ('label','target','class'):
            label_col = c
            break

print("Detected label column:", label_col)

if label_col:
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE, stratify=df[label_col])
else:
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)

print(f"Train rows: {len(train_df)}, Val rows: {len(val_df)}")

train_dataset = FundusDataset(train_df, TRAIN_IMG_DIR, fname_col=fname_col, label_col=label_col,
                             apply_clahe=True, apply_circular_crop=True, transform=train_transform)
val_dataset   = FundusDataset(val_df,   TRAIN_IMG_DIR, fname_col=fname_col, label_col=label_col,
                             apply_clahe=True, apply_circular_crop=True, transform=val_transform)

# compute class weights for imbalanced dataset (optional)
class_weights = None
class_weight_map = None
if label_col:
    counts = train_df[label_col].value_counts().sort_index().to_dict()
    print("Train label counts:", counts)
    labels_sorted = sorted(counts.keys())
    freq = np.array([counts[k] for k in labels_sorted], dtype=np.float32)
    class_weights = (1.0 / (freq + 1e-12))
    class_weights = class_weights / class_weights.sum() * len(freq)
    class_weight_map = {int(k): float(w) for k, w in zip(labels_sorted, class_weights)}
    print("Class weight map:", class_weight_map)
    sample_weights = train_df[label_col].map(lambda x: class_weight_map[int(x)]).values
    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)
else:
    sampler = None

# dataloaders
if sampler is not None:
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)
else:
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)

val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

print("Dataloaders ready. Train batches:", len(train_loader), "Val batches:", len(val_loader))

# Save small metadata
meta = {
    "img_size": IMG_SIZE,
    "batch_size": BATCH_SIZE,
    "train_rows": len(train_df),
    "val_rows": len(val_df),
    "fname_col": fname_col,
    "label_col": label_col,
    "class_weights": class_weight_map if class_weight_map is not None else None
}
meta_path = os.path.join(RESULTS_DIR, "dataset_meta_result2.json")
with open(meta_path, 'w') as f:
    json.dump(meta, f, indent=2)
print("Saved dataset meta to:", meta_path)

#quick visual sanity: show 3 sample images from train dataset (as tensors -> PIL)
def show_batch_samples(dl, n=3):
    batch = next(iter(dl))
    if isinstance(batch, (list,tuple)):
        imgs = batch[0][:n]
        labels = batch[1][:n]
    else:
        imgs = batch[:n]
        labels = None
    imgs = imgs.cpu().numpy()
    fig, axs = plt.subplots(1, n, figsize=(4*n,4))
    for i in range(n):
        im = imgs[i].transpose(1,2,0)
        im = im * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])
        im = np.clip(im, 0, 1)
        axs[i].imshow(im)
        axs[i].axis('off')
        if labels is not None:
            axs[i].set_title(f"label:{int(labels[i])}")
    show_or_save()

print("Showing sample train images (after transforms):")
show_batch_samples(train_loader, n=3)

# ----------------------------
# Added: show_by_class function
# ----------------------------
def show_by_class(dataset, classes=5):
    """
    Show one example image per class from the dataset (useful for imbalanced datasets).
    dataset: FundusDataset (not DataLoader), should return (img_tensor, label)
    classes: number of classes to show (e.g., 5 for 0..4)
    """
    seen = set()
    fig, axs = plt.subplots(1, classes, figsize=(4*classes,4))
    i = 0
    for item in dataset:
        if isinstance(item, (list,tuple)):
            img, label = item[0], item[1]
        else:
            continue
        if label not in seen:
            seen.add(label)
            im = img.cpu().numpy().transpose(1,2,0)
            im = im * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])
            im = np.clip(im, 0, 1)
            axs[i].imshow(im)
            axs[i].set_title(f"class {label}")
            axs[i].axis("off")
            i += 1
            if i == classes:
                break
    show_or_save()

#Call show_by_class to display one sample per class (0..4)
print("Showing one sample per class (0..4) from train dataset:")
show_by_class(train_dataset, classes=5)

# ===== Quick fix: ensure train_loader / val_loader exist =====
import os, torch
from torch.utils.data import DataLoader, WeightedRandomSampler

# 1) Quick check: do we already have train_loader/val_loader?
if 'train_loader' in globals() and 'val_loader' in globals():
    print("train_loader and val_loader already defined. You're good to run Cell 5.")
else:
    print("train_loader / val_loader not found. Attempting to construct them now...")

    # 2) Prefer using FundusDataset if already defined (from your Cell 3). Otherwise define a minimal fallback.
    use_existing_dataset = 'FundusDataset' in globals()
    if use_existing_dataset:
        DatasetClass = globals()['FundusDataset']
        print("Using existing FundusDataset class.")
    else:
        print("FundusDataset class not found — defining a minimal fallback dataset (SimpleFundusDataset).")
        from PIL import Image
        import pandas as pd
        import numpy as np

        class SimpleFundusDataset(torch.utils.data.Dataset):
            def __init__(self, df, images_dir, fname_col, label_col=None, transform=None):
                self.df = df.reset_index(drop=True)
                self.images_dir = images_dir
                self.fname_col = fname_col
                self.label_col = label_col if label_col in df.columns else None
                self.transform = transform

            def __len__(self):
                return len(self.df)

            def _resolve_path(self, name):
                exts = ['.png','.jpg','.jpeg','.tif','.tiff','.bmp']
                # try exact
                p = os.path.join(self.images_dir, str(name))
                if os.path.exists(p):
                    return p
                for e in exts:
                    p2 = os.path.join(self.images_dir, str(name) + e)
                    if os.path.exists(p2):
                        return p2
                # try lowercase + ext
                for e in exts:
                    p3 = os.path.join(self.images_dir, str(name).lower() + e)
                    if os.path.exists(p3):
                        return p3
                # fallback: return None
                return None

            def __getitem__(self, idx):
                row = self.df.iloc[idx]
                name = str(row[self.fname_col])
                path = self._resolve_path(name)
                if path is None:
                    raise FileNotFoundError(f"Image not found for {name} in {self.images_dir}")
                img = Image.open(path).convert("RGB")
                if self.transform:
                    img = self.transform(img)
                else:
                    import torchvision.transforms as T
                    img = T.ToTensor()(img)
                label = int(row[self.label_col]) if (self.label_col is not None and self.label_col in self.df.columns) else -1
                return img, label, os.path.basename(path)

        DatasetClass = SimpleFundusDataset

    # 3) Load df, TRAIN_IMG_DIR, LABEL_COL from globals if available
    if 'df' not in globals():
        raise RuntimeError("DataFrame `df` not found in globals. Run Cell 0 to load train.csv first.")
    if 'TRAIN_IMG_DIR' not in globals():
        raise RuntimeError("TRAIN_IMG_DIR not defined. Run Cell 0 to set paths.")

    # detect filename column similarly to your earlier logic
    df_local = globals()['df']
    possible_fname_cols = [c for c in df_local.columns if c.lower() in ('image','id','id_code','filename','image_name','image_id','img_name','file')]
    if len(possible_fname_cols) == 0:
        fname_col = df_local.columns[0]
        print("Filename col auto-chosen as first CSV column:", fname_col)
    else:
        fname_col = possible_fname_cols[0]
        print("Detected filename column:", fname_col)

    label_col = globals().get('LABEL_COL', None)
    if label_col is None:
        # fallback detection
        for c in df_local.columns:
            if c.lower() in ('diagnosis','label','target','class'):
                label_col = c
                break
    print("Using label column:", label_col)

    # 4) Re-create transforms used earlier if available, else simple transforms
    import torchvision.transforms as T
    if 'train_transform' in globals() and 'val_transform' in globals():
        t_train = globals()['train_transform']
        t_val = globals()['val_transform']
        print("Using existing train/val transforms.")
    else:
        print("train_transform/val_transform not found — creating simple transforms (Resize->ToTensor->Normalize).")
        IMG_SIZE = globals().get('IMG_SIZE', 224)
        IMAGENET_MEAN = [0.485,0.456,0.406]
        IMAGENET_STD  = [0.229,0.224,0.225]
        t_train = T.Compose([T.Resize((IMG_SIZE,IMG_SIZE)), T.RandomHorizontalFlip(), T.ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
        t_val = T.Compose([T.Resize((IMG_SIZE,IMG_SIZE)), T.ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])

    # 5) Train/val split (stratified if label_col exists)
    from sklearn.model_selection import train_test_split
    if label_col in df_local.columns:
        train_df, val_df = train_test_split(df_local, test_size=0.2, random_state=42, stratify=df_local[label_col])
    else:
        train_df, val_df = train_test_split(df_local, test_size=0.2, random_state=42)

    print(f"Train rows: {len(train_df)}, Val rows: {len(val_df)}")

    # 6) create datasets
    train_dataset = DatasetClass(train_df, globals()['TRAIN_IMG_DIR'], fname_col=fname_col, label_col=label_col, transform=t_train)
    val_dataset   = DatasetClass(val_df, globals()['TRAIN_IMG_DIR'], fname_col=fname_col, label_col=label_col, transform=t_val)

    # 7) sampler / class weights if available
    sampler = None
    class_weight_map = globals().get('class_weight_map', None)
    if class_weight_map is None and label_col in train_df.columns:
        # compute basic inverse-frequency weights
        counts = train_df[label_col].value_counts().to_dict()
        labels_sorted = sorted(counts.keys())
        freq = np.array([counts[k] for k in labels_sorted], dtype=float)
        cw = (1.0 / (freq + 1e-12))
        cw = cw / cw.sum() * len(freq)
        class_weight_map = {int(k): float(w) for k,w in zip(labels_sorted, cw)}
        print("Computed class_weight_map (fallback):", class_weight_map)
    if class_weight_map is not None:
        sample_weights = train_df[label_col].map(lambda x: class_weight_map.get(int(x), 1.0)).values
        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)
        print("Using WeightedRandomSampler for train loader.")

    # 8) create DataLoaders and assign to globals
    BATCH_SIZE = globals().get('BATCH_SIZE', 16)
    NUM_WORKERS = globals().get('NUM_WORKERS', 2)
    if sampler is not None:
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True)
    else:
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

    # 9) expose these globals so Cell 5 can use them
    globals()['train_loader'] = train_loader
    globals()['val_loader'] = val_loader
    globals()['train_dataset'] = train_dataset
    globals()['val_dataset'] = val_dataset
    globals()['fname_col'] = fname_col
    globals()['label_col'] = label_col
    globals()['class_weight_map'] = class_weight_map

    print("Created train_loader and val_loader successfully.")
    # 10) quick sanity: show one batch shapes
    batch = next(iter(train_loader))
    if isinstance(batch, (list,tuple)):
        imgs, labels = batch[0], batch[1]
    else:
        imgs, labels = batch, None
    print("Sample batch -> imgs.shape:", imgs.shape, "labels.shape:", getattr(labels, "shape", None))
    print("Now re-run Cell 5 (the training cell).")

# ===== Cell 4: Final TAHDL model (Multiscale CNN + 2-layer LSTM + Temporal Attention + Dual-FC heads) =====
import os, sys, json
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F

TS = datetime.now().strftime("%Y%m%d-%H%M%S")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Cell4 using device:", DEVICE)

# --- Try to reuse MultiscaleBackbone / CBAM / AdditiveAttention from previous cells if present ---
_reuse_backbone = 'MultiscaleBackbone' in globals()
_reuse_cbam = 'CBAM' in globals()
_reuse_additive = 'AdditiveAttention' in globals()

# If missing, define compact compatible MultiscaleBackbone and AdditiveAttention
if not _reuse_backbone:
    print("MultiscaleBackbone not found in globals — defining local MultiscaleBackbone (compatible).")
    class ConvBlock(nn.Module):
        def __init__(self, in_ch, out_ch, k=3, padding=None):
            super().__init__()
            if padding is None:
                padding = k//2
            self.net = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=k, padding=padding, bias=False),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )
        def forward(self,x): return self.net(x)

    class MultiscaleBackbone(nn.Module):
        def __init__(self, in_ch=3, base_filters=(32,64), branch_filters=(128,256), out_dim=768, return_intermediate=True):
            super().__init__()
            f1,f2 = base_filters
            bf1,bf2 = branch_filters
            self.return_intermediate = return_intermediate
            self.conv1 = ConvBlock(in_ch, f1, k=3)
            self.conv2 = ConvBlock(f1, f2, k=3)
            self.pool1 = nn.MaxPool2d(2,2)
            # branch1 (3x3)
            self.b1_1 = ConvBlock(f2, bf1, k=3); self.b1_2 = ConvBlock(bf1, bf2, k=3); self.b1_pool = nn.MaxPool2d(2,2)
            # branch2 (5x5)
            self.b2_1 = ConvBlock(f2, bf1, k=5, padding=2); self.b2_2 = ConvBlock(bf1, bf2, k=5, padding=2); self.b2_pool = nn.MaxPool2d(2,2)
            # branch3 (7x7)
            self.b3_1 = ConvBlock(f2, bf1, k=7, padding=3); self.b3_2 = ConvBlock(bf1, bf2, k=7, padding=3); self.b3_pool = nn.MaxPool2d(2,2)
            total_ch = bf2 * 3
            self.global_pool = nn.AdaptiveAvgPool2d((1,1))
            # projection to produce per-image embedding (embedding_dim)
            self.proj = nn.Sequential(nn.Flatten(), nn.Linear(total_ch, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(inplace=True))

        def forward(self, x):
            x = self.conv1(x); x = self.conv2(x); x = self.pool1(x)
            p1 = self.b1_1(x); p1 = self.b1_2(p1); p1 = self.b1_pool(p1)
            p2 = self.b2_1(x); p2 = self.b2_2(p2); p2 = self.b2_pool(p2)
            p3 = self.b3_1(x); p3 = self.b3_2(p3); p3 = self.b3_pool(p3)
            cat = torch.cat([p1,p2,p3], dim=1)
            pooled = self.global_pool(cat)
            emb = self.proj(pooled)
            if self.return_intermediate:
                return emb, cat
            return emb

if not _reuse_additive:
    print("AdditiveAttention not found — defining local AdditiveAttention (Bahdanau-like).")
    class AdditiveAttention(nn.Module):
        def __init__(self, dim, hidden_dim=None):
            super().__init__()
            if hidden_dim is None:
                hidden_dim = max(32, dim//2)
            self.w1 = nn.Linear(dim, hidden_dim, bias=False)
            self.v = nn.Linear(hidden_dim, 1, bias=False)
        def forward(self, seq_out, mask=None):
            # seq_out: (B, T, D)
            score = self.v(torch.tanh(self.w1(seq_out))).squeeze(-1)  # (B,T)
            if mask is not None:
                score = score.masked_fill(mask==0, float('-1e9'))
            weights = torch.softmax(score, dim=1)  # (B,T)
            context = torch.sum(seq_out * weights.unsqueeze(-1), dim=1)  # (B,D)
            return context, weights

# If CBAM is available, prefer using it for concat-feature attention; otherwise skip
use_cbam = _reuse_cbam
if use_cbam:
    print("CBAM detected in globals; will use CBAM on concat feature-maps.")

# -------- TAHDLModel (final) --------
class TAHDLModel(nn.Module):
    """
    Multiscale CNN -> per-image embedding -> 2-layer LSTM -> AdditiveAttention -> Dual FC heads -> Classifier
    Supports:
      - single image input: (B,3,H,W) -> treated as T=1 sequence
      - sequence input: (B,T,3,H,W)
    """
    def __init__(self, num_classes=5, embedding_dim=256, lstm_hidden=256, lstm_layers=2, dropout=0.5, bidirectional=False, use_cbam_flag=False):
        super().__init__()
        # instantiate backbone (match the one used in Cell3)
        # choose projection embedding_dim (per-image)
        # If you defined MultiscaleBackbone earlier with different out_dim, ensure embedding_dim matches or add a linear proj.
        # We'll instantiate MultiscaleBackbone with out_dim equal to embedding_dim if possible.
        try:
            # if previous MultiscaleBackbone signature supports out_dim/return_intermediate
            self.backbone = MultiscaleBackbone(in_ch=3, base_filters=(32,64), branch_filters=(128,256), out_dim=embedding_dim, return_intermediate=True)
        except Exception:
            # fallback: instantiate simpler backbone and add linear proj
            self.backbone = MultiscaleBackbone()  # may not have out_dim control
            # add proj to desired embedding_dim
            self._need_proj = True
            self.proj_after_backbone = nn.Linear(768, embedding_dim)
        self.embedding_dim = embedding_dim
        self.use_cbam_flag = use_cbam_flag and ('CBAM' in globals())
        if self.use_cbam_flag:
            # apply CBAM on the concatenated fmap produced by backbone if returned
            self.cbam = globals()['CBAM'](in_ch=256*3, reduction=16)

        # LSTM: input_size = embedding_dim
        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=lstm_hidden, num_layers=lstm_layers,
                            batch_first=True, bidirectional=bidirectional, dropout=0.5 if lstm_layers>1 else 0.0)
        self.lstm_hidden = lstm_hidden
        self.bidirectional = bidirectional
        self.num_directions = 2 if bidirectional else 1

        # Attention over temporal outputs
        self.attention = globals()['AdditiveAttention'](dim=lstm_hidden * self.num_directions) if 'AdditiveAttention' in globals() else AdditiveAttention(dim=lstm_hidden * self.num_directions)

        # FC heads per paper
        self.fc1 = nn.Sequential(nn.Linear(lstm_hidden * self.num_directions, 256), nn.ReLU(inplace=True))
        self.fc2 = nn.Sequential(nn.Linear(lstm_hidden * self.num_directions, 256), nn.ReLU(inplace=True))

        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )

    def forward(self, x, lengths=None):
        """
        x: (B,3,H,W) or (B,T,3,H,W)
        lengths: optional lengths for packed sequences (tensor of shape (B,))
        returns: logits (B,num_classes), attention_weights (B,T)
        """
        # convert single-image input to sequence of len 1
        if x.dim() == 4:
            x = x.unsqueeze(1)  # (B,1,3,H,W)

        B, T, C, H, W = x.shape
        # fuse batch and time for backbone pass
        xt = x.view(B*T, C, H, W)  # (B*T,3,H,W)
        # backbone may return (emb, fmap) when return_intermediate True
        backbone_out = self.backbone(xt)
        if isinstance(backbone_out, (tuple)) and len(backbone_out) >= 1:
            emb = backbone_out[0]   # (B*T, embedding_dim)
            fmap = backbone_out[1]  # (B*T, ch, h', w')  (optional)
            if self.use_cbam_flag:
                try:
                    fmap_att, attmaps = self.cbam(fmap, return_attentions=True)
                except Exception:
                    fmap_att = fmap
            else:
                fmap_att = fmap
        else:
            emb = backbone_out
            fmap_att = None

        # if we created a proj because backbone didn't support embedding_dim choice
        if hasattr(self, '_need_proj') and getattr(self, '_need_proj'):
            emb = self.proj_after_backbone(emb)

        # reshape to (B, T, embedding_dim)
        emb_seq = emb.view(B, T, -1)

        # pack padded sequence if lengths provided
        if lengths is not None:
            packed = nn.utils.rnn.pack_padded_sequence(emb_seq, lengths.cpu(), batch_first=True, enforce_sorted=False)
            packed_out, (h_n, c_n) = self.lstm(packed)
            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,hidden*dir)
        else:
            lstm_out, (h_n, c_n) = self.lstm(emb_seq)  # lstm_out: (B,T,hidden*dir)

        # last hidden (take last timestep)
        last_hidden = lstm_out[:, -1, :]  # (B, hidden*dir)

        # attention (context vector) over time
        context, attn_weights = self.attention(lstm_out)  # context: (B, hidden*dir), attn_weights: (B,T)

        # FC heads
        out1 = self.fc1(last_hidden)  # (B,256)
        out2 = self.fc2(context)      # (B,256)

        comb = torch.cat([out1, out2], dim=1)  # (B,512)
        logits = self.classifier(comb)         # (B, num_classes)

        return logits, attn_weights, fmap_att  # fmap_att optional (for visualization)

    def to_device(self, device=DEVICE):
        self.to(device)
        return self

# --------- instantiate model and print/save summary ---------
NUM_CLASSES = 5
EMB_DIM = 1024   # per-image embedding dimension (you can change to 768 if backbone was saved with 768)
model = TAHDLModel(num_classes=NUM_CLASSES, embedding_dim=EMB_DIM, lstm_hidden=512, lstm_layers=2, dropout=0.5, bidirectional=False, use_cbam_flag=use_cbam)
model.to(DEVICE)

def count_trainable_params(m):
    return sum(p.numel() for p in m.parameters() if p.requires_grad)

params = count_trainable_params(model)
print("TAHDLModel created. Trainable params:", params)

# Save an architecture summary file to RESULTS_DIR
arch_txt = os.path.join(RESULTS_DIR, f"model2_arch_summary_result2_{TS}.txt")
with open(arch_txt, 'w') as f:
    f.write("TAHDLModel architecture summary\n")
    f.write(f"Created: {TS}\n")
    f.write(f"Device: {DEVICE}\n")
    f.write(f"Num classes: {NUM_CLASSES}\n")
    f.write(f"Embedding dim: {EMB_DIM}\n")
    f.write(f"LSTM hidden: {model.lstm_hidden}, layers: {model.lstm.num_layers}, bidir: {model.bidirectional}\n")
    f.write(f"Use CBAM: {model.use_cbam_flag}\n")
    f.write(f"Trainable params: {params}\n")
print("Saved architecture summary to:", arch_txt)

# --------- Smoke tests: dummy inputs ---------
with torch.no_grad():
    # single image (B,3,H,W)
    d1 = torch.randn(2,3,224,224).to(DEVICE)
    logits1, att1, fmap1 = model(d1)
    print("Single-image forward -> logits:", logits1.shape, "attn:", att1.shape, "fmap_att:", None if fmap1 is None else fmap1.shape)

    # sequence input (B,T,3,H,W)
    d2 = torch.randn(2, 3, 3, 224, 224).to(DEVICE)  # B=2, T=3
    logits2, att2, fmap2 = model(d2)
    print("Sequence forward -> logits:", logits2.shape, "attn:", att2.shape, "fmap_att:", None if fmap2 is None else fmap2.shape)

# If you have a small loader variable available, try one batch through model (non-blocking)
if 'loader' in globals():
    try:
        batch = next(iter(loader))
        if isinstance(batch, (list,tuple)):
            imgs = batch[0]
        else:
            imgs = batch
        # If loader returns (B,3,H,W) or (B, T, 3,H,W) adapt
        imgs = imgs.to(DEVICE)
        model.eval()
        with torch.no_grad():
            out_logits, out_att, out_fmap = model(imgs)
        print("Loader smoke forward OK -> logits shape:", out_logits.shape)
    except Exception as e:
        print("Loader smoke-test failed:", e)

print("Cell 4 done. Model saved summary ->", arch_txt)



# ===== Cell 6: Training Loop (Corrected & Complete) =====
import os, time, json, math
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report
import pandas as pd
import matplotlib.pyplot as plt


# ---- Preconditions ----
if 'train_loader' not in globals() or 'val_loader' not in globals():
    raise RuntimeError("train_loader/val_loader not found. Run dataloader cell first.")
if 'model' not in globals():
    raise RuntimeError("Model `model` not found. Run model cell (Cell 4).")
if 'MODEL_DIR' not in globals() or 'RESULTS_DIR' not in globals():
    raise RuntimeError("MODEL_DIR or RESULTS_DIR not set (from Cell 0).")


# ---- Config / resume toggle ----
run_cfg = globals().get('run_cfg', globals().get('run_config', {}))
EPOCHS = int(run_cfg.get('epochs', 5))
LR = float(run_cfg.get('lr', 0.001))
CLIP_GRAD_NORM = float(run_cfg.get('clip_grad_norm', 2.0))
RESUME = bool(run_cfg.get('resume', False))  # set True in run_cfg to enable resume
MONITOR = "val_macro_f1"


DEVICE = globals().get('DEVICE', torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print("Device:", DEVICE)
if DEVICE.type == 'cpu':
    print("⚠️ Running on CPU — will be slow. Use GPU if available.")


model = model.to(DEVICE)


# ---- optimizer / scheduler / scaler ----
optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=float(run_cfg.get('weight_decay', 0.01)))
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))


# ---- criterion ----
criterion = globals().get('criterion', None)
if criterion is None:
    criterion = torch.nn.CrossEntropyLoss()
    print("No criterion found in globals; using default CrossEntropyLoss")


# ---- Resume support ----
TIMESTAMP = datetime.now().strftime("%Y%m%d-%H%M%S")
best_val_macro_f1 = -1.0
best_ckpt_path = None
start_epoch = 1


if RESUME:
    last_candidates = sorted([p for p in os.listdir(MODEL_DIR)
                             if p.startswith("last_model_result2_") and p.endswith(".pth")])
    if last_candidates:
        last_ckpt = os.path.join(MODEL_DIR, last_candidates[-1])
        print("Attempting to resume from:", last_ckpt)
        ck = torch.load(last_ckpt, map_location=DEVICE)
        if 'model_state_dict' in ck:
            model.load_state_dict(ck['model_state_dict'])
        if 'optimizer_state_dict' in ck:
            try:
                optimizer.load_state_dict(ck['optimizer_state_dict'])
            except Exception as e:
                print("Warning: couldn't load optimizer state:", e)
        start_epoch = int(ck.get('epoch', 0)) + 1
        print("Resumed. Starting epoch:", start_epoch)
    else:
        print("RESUME requested but no last_model found.")


# ---- Metadata ----
meta_common = {
    "seed": int(run_cfg.get('seed', 42)),
    "run_cfg": run_cfg,
    "class_order": globals().get('class_weight_map', None) and sorted(list(globals()['class_weight_map'].keys())) or None
}


# ---- History ----
history_path = os.path.join(RESULTS_DIR, f"history_result2_{TIMESTAMP}.json")
history = {
    "train_loss": [], "val_loss": [],
    "train_acc": [], "val_acc": [],
    "val_macro_f1": [], "val_weighted_f1": [],
    "lr": [], "epoch_time": []
}


# ---- Helper: evaluate and save predictions ----
softmax = nn.Softmax(dim=1)


def evaluate_and_save_preds(model, dataloader, device, epoch, results_dir, ts):
    """Evaluate model and save predictions CSV"""
    model.eval()
    y_true = []
    y_pred = []
    y_probs = []
    ids_list = []
    total_loss = 0.0
    n = 0

    with torch.no_grad():
        for batch in dataloader:
            # Unpack batch (support 2 or 3 items)
            if len(batch) == 2:
                imgs, labels = batch[0].to(device), batch[1].to(device)
                ids = None
            elif len(batch) == 3:
                imgs, labels, ids = batch[0].to(device), batch[1].to(device), batch[2]
            else:
                imgs, labels = batch[0].to(device), batch[1].to(device)
                ids = None

            # Forward pass
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                out = model(imgs)
                logits = out[0] if isinstance(out, (tuple, list)) else out
                loss = criterion(logits, labels)

            probs = softmax(logits).cpu().numpy()
            preds = logits.argmax(dim=1).cpu().numpy()

            y_true.extend(labels.cpu().numpy().tolist())
            y_pred.extend(preds.tolist())
            y_probs.extend(probs.tolist())

            if ids is not None:
                ids_list.extend([str(x) for x in ids])
            else:
                ids_list.extend([None]*len(preds))

            total_loss += loss.item() * labels.size(0)
            n += labels.size(0)

    avg_loss = total_loss / n if n > 0 else 0.0
    macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    acc = accuracy_score(y_true, y_pred)

    # Save predictions CSV
    preds_df = pd.DataFrame({
        "id": ids_list,
        "true": y_true,
        "pred": y_pred,
        "probabilities": y_probs
    })
    preds_path = os.path.join(results_dir, f"predictions_epoch{epoch}_{ts}.csv")
    preds_df.to_csv(preds_path, index=False)
    print(f"Saved predictions for epoch {epoch} to: {preds_path}")

    return avg_loss, acc, macro, weighted, np.array(y_true), np.array(y_pred), preds_path


# ===== MAIN TRAINING LOOP =====
best_val_macro_f1 = -1.0
best_meta = None


for epoch in range(start_epoch, EPOCHS + 1):
    t0 = time.time()
    model.train()
    running_loss = 0.0
    running_correct = 0
    total = 0

    pbar = tqdm(train_loader, desc=f"Train epoch {epoch}/{EPOCHS}")
    for batch in pbar:
        # Unpack batch
        imgs = batch[0].to(DEVICE)
        labels = batch[1].to(DEVICE)

        optimizer.zero_grad()

        # Forward pass
        with torch.amp.autocast('cuda', enabled=(DEVICE.type=='cuda')):
            out = model(imgs)
            logits = out[0] if isinstance(out, (tuple, list)) else out
            loss = criterion(logits, labels)

        # Backward pass
        if DEVICE.type == 'cuda':
            scaler.scale(loss).backward()
            if CLIP_GRAD_NORM and CLIP_GRAD_NORM > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            if CLIP_GRAD_NORM and CLIP_GRAD_NORM > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)
            optimizer.step()

        preds = logits.argmax(dim=1)
        bs = labels.size(0)
        running_loss += loss.item() * bs
        running_correct += (preds == labels).sum().item()
        total += bs

        lr_now = optimizer.param_groups[0]['lr']
        pbar.set_postfix({'loss': loss.item(), 'lr': lr_now})

    # Train epoch stats
    train_loss = running_loss / total if total > 0 else 0.0
    train_acc = running_correct / total if total > 0 else 0.0

    # Validation + save predictions
    val_loss, val_acc, val_macro_f1, val_weighted_f1, y_true, y_pred, preds_path = \
        evaluate_and_save_preds(model, val_loader, DEVICE, epoch, RESULTS_DIR, TIMESTAMP)

    # Scheduler step
    try:
        scheduler.step(val_macro_f1)
    except Exception as e:
        print("Scheduler step error; falling back to val_acc. Err:", e)
        scheduler.step(val_acc)

    epoch_time = time.time() - t0
    lr_now = optimizer.param_groups[0]['lr']

    # Update history
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_acc'].append(train_acc)
    history['val_acc'].append(val_acc)
    history['val_macro_f1'].append(val_macro_f1)
    history['val_weighted_f1'].append(val_weighted_f1)
    history['lr'].append(lr_now)
    history['epoch_time'].append(epoch_time)

    # Save history
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    print(f"Epoch {epoch} summary: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, "
          f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_macro_f1={val_macro_f1:.4f}, "
          f"val_weighted_f1={val_weighted_f1:.4f}, lr={lr_now:.6g}, time={epoch_time:.1f}s")

    # Checkpointing - save last model
    last_ckpt_path = os.path.join(MODEL_DIR, f"last_model_result2_{TIMESTAMP}.pth")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'history': history,
        'run_cfg': run_cfg,
        'timestamp': TIMESTAMP
    }, last_ckpt_path)

    # Save best model if improved
    if val_macro_f1 > best_val_macro_f1:
        best_val_macro_f1 = val_macro_f1
        best_name = f"best_model_result2_macroF1_{best_val_macro_f1:.4f}_{TIMESTAMP}.pth"
        best_ckpt_path = os.path.join(MODEL_DIR, best_name)
        meta = {**meta_common, "epoch": epoch, "val_macro_f1": float(val_macro_f1), "preds_csv": preds_path}
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'history': history,
            'metadata': meta,
            'run_cfg': run_cfg,
            'timestamp': TIMESTAMP
        }, best_ckpt_path)
        best_meta = meta
        print(f"Saved BEST checkpoint -> {best_ckpt_path}")




# ===== TRAINING PLOTS =====
train_plot_path = os.path.join(RESULTS_DIR, f"training_plots_{TIMESTAMP}.png")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Loss plot
ax1.plot(history['train_loss'], label='train_loss', marker='o')
ax1.plot(history['val_loss'], label='val_loss', marker='s')
ax1.set_title('Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Accuracy plot
ax2.plot(history['train_acc'], label='train_acc', marker='o')
ax2.plot(history['val_acc'], label='val_acc', marker='s')
ax2.set_title('Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(train_plot_path, dpi=150, bbox_inches='tight')
show_or_save()
print(f"Saved training plots to: {train_plot_path}")


# ===== RUN SUMMARY =====
# This block is removed from Cell 6 as its variables are defined in Cell 7
summary = {
    "timestamp": TIMESTAMP,
    "best_ckpt": best_ckpt_path,
    "last_ckpt": last_ckpt_path,
    "history_path": history_path,
    "metrics_path": metrics_path,
    "confusion_matrix": cm_path,
    "report_path": report_path,
    "training_plots": train_plot_path
}
summary_path = os.path.join(RESULTS_DIR, f"summary_result2_{TIMESTAMP}.json")
with open(summary_path, 'w') as f:
    json.dump(summary, f, indent=2)
print(f"Saved run summary to: {summary_path}")

print("\n" + "="*70)
print("✅ Cell 6 complete — training+validation finished!")
print("="*70)


# ===== Cell 7: Final evaluation, confusion matrices, predictions CSV, error thumbnails, append summary =====
import os, json, math, glob
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score
from PIL import Image, ImageDraw, ImageFont

# --- Preconditions ---
if 'val_loader' not in globals():
    raise RuntimeError("val_loader missing. Run Cell 3 to create dataloaders.")
if 'MODEL_DIR' not in globals() or 'RESULTS_DIR' not in globals():
    raise RuntimeError("MODEL_DIR or RESULTS_DIR missing (Cell0).")

DEVICE = globals().get('DEVICE', torch.device("cuda" if torch.cuda.is_available() else "cpu"))
model = globals().get('model', None)
if model is None:
    raise RuntimeError("Model object not found in globals. Load/instantiate model before running this cell.")
model = model.to(DEVICE)
criterion = globals().get('criterion', nn.CrossEntropyLoss())

TS = datetime.now().strftime("%Y%m%d-%H%M%S")
OUTDIR = os.path.join(RESULTS_DIR, f"eval_result2_{TS}")
os.makedirs(OUTDIR, exist_ok=True)

# helper: find best checkpoint (most recent best_model_result2_macroF1_*.pth)
best_ckpts = sorted(glob.glob(os.path.join(MODEL_DIR, "best_model_result2_macroF1_*.pth")))
if len(best_ckpts) == 0:
    print("No best checkpoint found in MODEL_DIR. Trying last_model_result2_*.pth")
    last_ckpts = sorted(glob.glob(os.path.join(MODEL_DIR, "last_model_result2_*.pth")))
    if len(last_ckpts)==0:
        raise FileNotFoundError("No checkpoint found to evaluate. Run training first.")
    ckpt_path = last_ckpts[-1]
else:
    ckpt_path = best_ckpts[-1]

print("Loading checkpoint for evaluation:", ckpt_path)
ck = torch.load(ckpt_path, map_location=DEVICE)
if 'model_state_dict' in ck:
    model.load_state_dict(ck['model_state_dict'])
else:
    # assume state_dict saved directly
    try:
        model.load_state_dict(ck)
    except Exception as e:
        print("Warning: couldn't load checkpoint via model_state_dict, trying direct load:", e)
        raise

model.eval()
softmax = nn.Softmax(dim=1)

# iterate over val_loader and collect predictions
y_trues = []
y_preds = []
y_probs = []
ids_all = []
loss_sum = 0.0
n_all = 0

# optional: capture attention maps if model returns them
save_attention_maps = True
attention_examples = []  # (id, true, pred, attention_map numpy)

with torch.no_grad():
    for batch in val_loader:
        # support batches of formats: (imgs, labels), (imgs, labels, ids), etc.
        if isinstance(batch, (list,tuple)):
            if len(batch) == 2:
                imgs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)
                ids = [None]*labels.size(0)
            elif len(batch) == 3:
                imgs, labels, ids = batch[0].to(DEVICE), batch[1].to(DEVICE), batch[2]
                # ensure ids list of strings
                try:
                    ids = [str(x) for x in ids]
                except:
                    ids = [str(x) for x in ids]
            elif len(batch) == 4:
                # common sequence mode: (imgs_seq, labels, lengths, ids)
                imgs, labels, lengths, ids = batch
                imgs = imgs.to(DEVICE)
                labels = labels.to(DEVICE)
                try:
                    ids = [str(x) for x in ids]
                except:
                    ids = [str(x) for x in ids]
            else:
                imgs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)
                ids = [None]*labels.size(0)
        else:
            raise RuntimeError("Unexpected batch format from val_loader.")

        out = model(imgs)
        if isinstance(out, (tuple, list)) and len(out) >= 1:
            logits = out[0]
            attn_map = out[1] if len(out) > 1 else None
        else:
            logits = out
            attn_map = None

        loss = criterion(logits, labels)
        probs = softmax(logits).cpu().numpy()
        preds = logits.argmax(dim=1).cpu().numpy()
        labels_np = labels.cpu().numpy()

        # accumulate
        y_trues.extend(labels_np.tolist())
        y_preds.extend(preds.tolist())
        y_probs.extend(probs.tolist())
        ids_all.extend(ids)
        loss_sum += loss.item() * labels.size(0)
        n_all += labels.size(0)

        # save attention examples for a few mismatch/correct examples
        if save_attention_maps and (attn_map is not None):
            # attn_map shape expected (B, T, T) or (B, T). We'll save the raw map per-sample.
            # Assuming attn_map is a 1D tensor of attention weights for a sequence of images.
            try:
                attn_np = attn_map.detach().cpu().numpy()
                # For now, just save it directly. Visualization logic will handle shape.
                B = attn_np.shape[0]
                for i in range(B):
                    if len(attention_examples) < 200:  # cap to avoid memory blow
                        attention_examples.append((ids[i] if ids else None, int(labels_np[i]), int(preds[i]), attn_np[i]))
            except Exception:
                pass

# compute metrics
avg_loss = loss_sum / n_all if n_all>0 else 0.0
acc = accuracy_score(y_trues, y_preds)
macro_f1 = f1_score(y_trues, y_preds, average='macro', zero_division=0)
weighted_f1 = f1_score(y_trues, y_preds, average='weighted', zero_division=0)
clf_report = classification_report(y_trues, y_preds, digits=4, zero_division=0)
cm = confusion_matrix(y_trues, y_preds)

print("Validation results -> loss: {:.4f}, acc: {:.4f}, macro_f1: {:.4f}, weighted_f1: {:.4f}".format(avg_loss, acc, macro_f1, weighted_f1))
print("Classification report:\n", clf_report)

# --- Save metrics JSON ---
metrics = {
    "checkpoint": os.path.basename(ckpt_path),
    "avg_loss": float(avg_loss),
    "accuracy": float(acc),
    "macro_f1": float(macro_f1),
    "weighted_f1": float(weighted_f1),
    "n_samples": int(n_all),
    "timestamp": TS
}
metrics_path = os.path.join(OUTDIR, f"metrics_result2_{TS}.json")
with open(metrics_path, 'w') as f:
    json.dump(metrics, f, indent=2)
print("Saved metrics JSON:", metrics_path)

# --- Save predictions CSV (top-3 probs) ---
topk = 3
probs_arr = np.array(y_probs)
topk_idx = np.argsort(-probs_arr, axis=1)[:, :topk]
topk_vals = -np.sort(-probs_arr, axis=1)[:, :topk]
preds_df = []
for i, (idv, true, pred, probs_row, idxs, vals) in enumerate(zip(ids_all, y_trues, y_preds, probs_arr, topk_idx, topk_vals)):
    topk_pairs = ";".join([f"{int(idxs[j])}:{vals[j]:.4f}" for j in range(len(idxs))])
    preds_df.append({"id": idv, "true": int(true), "pred": int(pred), "topk": topk_pairs, "probs": probs_row.tolist()})
preds_df = pd.DataFrame(preds_df)
preds_csv = os.path.join(OUTDIR, f"predictions_top{topk}_{TS}.csv")
preds_df.to_csv(preds_csv, index=False)
print("Saved predictions CSV (top-k):", preds_csv)

# --- Save confusion matrix (PNG + SVG) and normalized matrix ---
cm_png = os.path.join(OUTDIR, f"confmat_{TS}.png")
cm_svg = os.path.join(OUTDIR, f"confmat_{TS}.svg")
plt.figure(figsize=(7,6))
sns.set(style="whitegrid")
ax = sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", cbar=True, linewidths=0.5, linecolor='white')
ax.set_xlabel("Predicted")
ax.set_ylabel("True")
ax.set_title("Confusion Matrix (val)")
plt.tight_layout()
plt.savefig(cm_png, dpi=300, bbox_inches="tight")
plt.savefig(cm_svg, bbox_inches="tight")
show_or_save()
print("Saved confusion matrix images:", cm_png, cm_svg)

# normalized (row-wise)
row_sums = cm.sum(axis=1, keepdims=True).astype(float)
row_sums[row_sums==0] = 1.0
cm_norm = cm.astype(float) / row_sums
cmn_png = os.path.join(OUTDIR, f"confmat_normalized_{TS}.png")
plt.figure(figsize=(7,6))
axn = sns.heatmap(cm_norm, annot=True, fmt=".2f", cmap="Blues", cbar=True, linewidths=0.5, linecolor='white')
axn.set_xlabel("Predicted")
axn.set_ylabel("True")
axn.set_title("Normalized Confusion Matrix (row-wise)")
plt.tight_layout()
plt.savefig(cmn_png, dpi=300, bbox_inches="tight")
show_or_save()
print("Saved normalized confusion matrix:", cmn_png)

# --- Save classification report text ---
report_txt = os.path.join(OUTDIR, f"classification_report_{TS}.txt")
with open(report_txt, 'w') as f:
    f.write(clf_report)
print("Saved classification report to:", report_txt)

# Already saved above.

# --- Save misclassified thumbnails (FP / FN) for quick inspection ---
# We'll create small folders and save up to N examples per category
N_examples = 50
err_dir = os.path.join(OUTDIR, "error_cases")
os.makedirs(err_dir, exist_ok=True)
fp_dir = os.path.join(err_dir, "false_positives")
fn_dir = os.path.join(err_dir, "false_negatives")
tp_dir = os.path.join(err_dir, "true_positives")
tn_dir = os.path.join(err_dir, "true_negatives")
for d in (fp_dir, fn_dir, tp_dir, tn_dir):
    os.makedirs(d, exist_ok=True)

# We need mapping from id -> file path. Try to recover from val_loader dataset if possible
# If ids_all contains file basenames, attempt to find path under TRAIN_IMG_DIR or DATASET_DIR. Otherwise skip saving.
dataset_paths = {}
try:
    # if val_loader.dataset returns (img,label,filename) then we can map
    if hasattr(val_loader.dataset, 'df') and hasattr(val_loader.dataset, 'images_dir'):
        df_val = val_loader.dataset.df
        fname_col = getattr(val_loader.dataset, 'fname_col', None)
        if fname_col is not None:
            for _, row in df_val.iterrows():
                key = str(row[fname_col])
                # resolve exact path heuristics (try extensions)
                resolved = None
                for ext in ['.png','.jpg','.jpeg','.tif','.tiff']:
                    p = os.path.join(val_loader.dataset.images_dir, key)
                    if os.path.exists(p):
                        resolved = p; break
                    p2 = os.path.join(val_loader.dataset.images_dir, key + ext)
                    if os.path.exists(p2):
                        resolved = p2; break
                    p3 = os.path.join(val_loader.dataset.images_dir, str(key).lower() + ext)
                    if os.path.exists(p3):
                        resolved = p3; break
                if resolved:
                    dataset_paths[key] = resolved
except Exception:
    pass

# now iterate predictions and save thumbnails
saved_fp = saved_fn = saved_tp = saved_tn = 0
for i, (idv, true_v, pred_v) in enumerate(zip(ids_all, y_trues, y_preds)):
    # try to find file
    fp = None
    if idv:
        # idv may be full filename or id. try direct mapping
        if idv in dataset_paths:
            fp = dataset_paths[idv]
        else:
            # maybe idv is filename with ext already
            for root in [globals().get('TRAIN_IMG_DIR'), globals().get('TEST_IMG_DIR'), globals().get('DATASET_DIR')]:
                if root is None: continue
                cand = os.path.join(root, str(idv))
                if os.path.exists(cand):
                    fp = cand; break
                # try appended ext
                for ext in ['.png','.jpg','.jpeg','.tif','.tiff']:
                    cand2 = os.path.join(root, str(idv)+ext)
                    if os.path.exists(cand2):
                        fp = cand2; break
                if fp: break
    # skip if not found
    if fp is None:
        continue
    try:
        img = Image.open(fp).convert("RGB")
    except:
        continue

    # annotate small label text on thumbnail
    draw = ImageDraw.Draw(img)
    txt = f"t:{true_v} p:{pred_v}"
    try:
        # try default font
        font = ImageFont.load_default()
    except:
        font = None
    draw.text((5,5), txt, fill=(255,255,255), font=font)

    # categorize
    if pred_v != true_v:
        # error
        # false positive = predicted class > true class? that's ambiguous for multi-class; we instead save by (pred != true)
        # Save both FP and FN relative to each class; here we will save as "false" bucket
        # To be more precise, save in FP if pred > true else FN (not strictly meaningful for DR)
        if pred_v > true_v:
            if saved_fp < N_examples:
                outp = os.path.join(fp_dir, f"{i}_id_{idv}_t{true_v}_p{pred_v}.png")
                img.save(outp)
                saved_fp += 1
        else:
            if saved_fn < N_examples:
                outp = os.path.join(fn_dir, f"{i}_id_{idv}_t{true_v}_p{pred_v}.png")
                img.save(outp)
                saved_fn += 1
    else:
        # correct
        if saved_tp < N_examples:
            outp = os.path.join(tp_dir, f"{i}_id_{idv}_t{true_v}.png")
            img.save(outp)
            saved_tp += 1

# print summary of saved thumbnails
print(f"Saved thumbnails -> FP:{saved_fp}, FN:{saved_fn}, TP:{saved_tp} to {err_dir}")

# --- Save attention maps if present (a few examples) ---
attn_outdir = os.path.join(OUTDIR, "attention_maps")
os.makedirs(attn_outdir, exist_ok=True)
if len(attention_examples) > 0:
    # save up to 50 attention maps as images (heatmap)
    max_attn_save = 50
    for idx, (idv, t, p, attn_np) in enumerate(attention_examples[:max_attn_save]):
        try:
            # attn_np shape may be (T,T) or (T,) or (H,W). We visualize sensible shapes.
            # We'll collapse to 2D if possible.
            arr = np.array(attn_np)
            if arr.ndim == 1:
                # make 2D by outer product
                viz = np.outer(arr, arr)
            elif arr.ndim == 2:
                viz = arr
            else:
                viz = arr.reshape(arr.shape[0], -1)
            plt.figure(figsize=(4,4))
            sns.heatmap(viz, cmap='viridis', cbar=True)
            plt.title(f"attn id={idv} t={t} p={p}")
            fpath = os.path.join(attn_outdir, f"attn_{idx}_id_{idv}_t{t}_p{p}.png")
            plt.savefig(fpath, bbox_inches='tight', dpi=150)
            plt.close()
        except Exception:
            pass
    print("Saved attention maps (samples) to:", attn_outdir)
else:
    print("No attention maps found in model outputs (or none captured).")

# --- Append summary row to comparison_table.csv in RESULTS_DIR (create if missing) ---
comp_csv = os.path.join(RESULTS_DIR, "comparison_table.csv")
row = {
    "timestamp": TS,
    "checkpoint": os.path.basename(ckpt_path),
    "val_loss": float(avg_loss),
    "val_acc": float(acc),
    "val_macro_f1": float(macro_f1),
    "val_weighted_f1": float(weighted_f1),
    "n_val": int(n_all)
}
# try to include run_cfg fields like epochs, lr
try:
    row.update({
        "epochs": int(run_cfg.get('epochs', None)),
        "lr": float(run_cfg.get('lr', None)),
        "batch_size": int(run_cfg.get('batch_size', None))
    })
except:
    pass

if os.path.exists(comp_csv):
    try:
        df_comp = pd.read_csv(comp_csv)
        df_comp = df_comp.append(row, ignore_index=True)
    except Exception:
        df_comp = pd.DataFrame([row])
else:
    df_comp = pd.DataFrame([row])
df_comp.to_csv(comp_csv, index=False)
print("Appended run summary to:", comp_csv)

# --- final print & top-level paths ---
print("\nEvaluation complete. Outputs saved under:", OUTDIR)
print("  - metrics json:", metrics_path)
print("  - predictions csv:", preds_csv)
print("  - confusion matrix png/svg:", cm_png, cm_svg)
print("  - normalized confusion matrix:", cmn_png)
print("  - classification report:", report_txt)
print("  - error thumbnails folder:", err_dir)
print("  - attention maps (if any):", attn_outdir)
print("  - comparison table updated:", comp_csv)


#cell7 complete

# ========== Cell 8: Professional Visualization & Result Summary ==========

# -- Imports (you need these for all the analytics and plots) --
import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve

# -- Helper: Fetch latest results based on timestamp for safety --
def get_latest_file(folder, prefix, ext):
    files = [f for f in os.listdir(folder) if f.startswith(prefix) and f.endswith(ext)]
    if not files:
        raise FileNotFoundError(f"No file {prefix}*.{ext} found in {folder}")
    return sorted(files)[-1]

# Find the latest OUTDIR from Cell 7's execution
latest_eval_dirs = sorted([d for d in os.listdir(RESULTS_DIR) if d.startswith("eval_result2_") and os.path.isdir(os.path.join(RESULTS_DIR, d))])
if not latest_eval_dirs:
    raise FileNotFoundError("No eval_result2_* directory found in RESULTS_DIR. Please ensure Cell 7 ran successfully.")
LATEST_OUTDIR = os.path.join(RESULTS_DIR, latest_eval_dirs[-1])

# Now get the paths using the correct directories and prefixes
history_fp = os.path.join(RESULTS_DIR, get_latest_file(RESULTS_DIR, "history_result2_", ".json")) # History is saved directly in RESULTS_DIR
metrics_fp = os.path.join(LATEST_OUTDIR, get_latest_file(LATEST_OUTDIR, "metrics_result2_", ".json"))
report_fp  = os.path.join(LATEST_OUTDIR, get_latest_file(LATEST_OUTDIR, "classification_report_", ".txt")) # Corrected prefix
cm_img_fp  = os.path.join(LATEST_OUTDIR, get_latest_file(LATEST_OUTDIR, "confmat_", ".png")) # Corrected prefix

with open(history_fp) as f: history = json.load(f)
with open(metrics_fp) as f: metrics = json.load(f)
with open(report_fp) as f: report_txt = f.read()

# -- 1. Training Process Curves (Loss, Accuracy, Macro-F1, Weighted-F1) --
plt.figure(figsize=(18,5))

plt.subplot(1,3,1)
plt.plot(history['train_loss'], label='Train Loss', color='orange')
plt.plot(history['val_loss'], label='Val Loss', color='blue')
plt.title("Train/Val Loss (per epoch)")
plt.xlabel("Epoch"); plt.ylabel("Loss")
plt.legend(); plt.grid(True)

plt.subplot(1,3,2)
plt.plot(history['train_acc'], label='Train Accuracy', color='seagreen')
plt.plot(history['val_acc'], label='Val Accuracy', color='darkmagenta')
plt.title("Train/Val Accuracy (per epoch)")
plt.xlabel("Epoch"); plt.ylabel("Accuracy")
plt.legend(); plt.grid(True)

plt.subplot(1,3,3)
plt.plot(history['val_macro_f1'], label='Macro F1', color='crimson')
plt.plot(history['val_weighted_f1'], label='Weighted F1', color='navy')
plt.title("Val Macro/Weighted F1 (per epoch)")
plt.xlabel("Epoch")
plt.ylabel("F1 Score")
plt.legend(); plt.grid(True)

plt.tight_layout()
show_or_save()

# -- 2. Final Classification Report (Table + Barplot) --
print("========== FINAL CLASSIFICATION REPORT (Per Disease Stage) ==========")
print(report_txt)

# Parse per-class F1 from classification report
import re
lines = report_txt.split('\n')
class_names, class_prec, class_rec, class_f1 = [], [], [], []
for l in lines:
    parts = re.split(r'\s{2,}', l.strip())
    if len(parts) >= 4 and parts[0].isdigit():
        class_names.append(parts[0])
        class_prec.append(float(parts[1]))
        class_rec.append(float(parts[2]))
        class_f1.append(float(parts[3]))

# -- 3. Per-class F1 Comparison (Barplot): Disease-wise F1 Scores --
plt.figure(figsize=(7,5))
plt.bar(
    ['No DR','Mild','Moderate','Severe','Proliferative'],
    class_f1, color=sns.color_palette("Blues", 5))
plt.title("Per-Class/Disease F1-score (TAHDL)")
plt.ylim(0,1)
for i, v in enumerate(class_f1):
    plt.text(i, v+0.02, f"{v:.2f}", ha='center')
plt.ylabel('F1-score'); plt.xlabel('Class/Disease Stage')
show_or_save()

# -- 4. Confusion Matrix Visual (Direct from PNG) --
from PIL import Image
im = Image.open(cm_img_fp)
plt.figure(figsize=(6,6)); plt.imshow(im); plt.axis('off')
plt.title("Confusion Matrix (TAHDL, Val Set)")
show_or_save()

# -- 5. Error/Confusion Analysis Heatmap --

# However, given current execution flow, `cm` is present in globals.
if 'cm' in globals():
    conf_matrix = globals()['cm']
else:
   
    print("Warning: `cm` (confusion matrix numpy array) not found in globals. Re-computing from predictions if available.")
    
    conf_matrix = np.eye(len(class_names), dtype=int) # Placeholder if no real CM available

df_cm = pd.DataFrame(conf_matrix, index=['No DR','Mild','Moderate','Severe','Proliferative'],
                                    columns=['No DR','Mild','Moderate','Severe','Proliferative'])
plt.figure(figsize=(7,5))
sns.heatmap(df_cm, annot=True, fmt='d', cmap="YlGnBu")
plt.title("Confusion Matrix Heatmap (Val Set, TAHDL)")
plt.ylabel("True Class"); plt.xlabel("Predicted Class")
show_or_save()

# -- 6. Precision-Recall Curve per Class (if probabilities are available) --
# The probabilities are saved in `predictions_topk_*.csv`. We need to load it.
# Assuming the `preds_csv` variable from Cell 7 stores the latest predictions CSV path
preds_csv_files = sorted(glob.glob(os.path.join(LATEST_OUTDIR, "predictions_top3_*.csv")))
if preds_csv_files:
    preds_df_for_pr_roc = pd.read_csv(preds_csv_files[-1])
    y_true_pr_roc = preds_df_for_pr_roc['true'].values
    # Probabilities are stored as strings of lists, need to parse them.
    y_proba_str = preds_df_for_pr_roc['probs'].apply(json.loads).tolist()
    y_proba = np.array(y_proba_str)

    from sklearn.preprocessing import label_binarize
    n_classes = y_proba.shape[1]
    plt.figure(figsize=(8,7))
    for i in range(n_classes):
        precision, recall, _ = precision_recall_curve(
            label_binarize(y_true_pr_roc, classes=list(range(n_classes)))[:,i],
            y_proba[:,i]
        )
        pr_auc = auc(recall, precision)
        plt.plot(recall, precision, label=f"{['No DR','Mild','Mod.','Severe','Prolif.'][i]} (AUC={pr_auc:.2f})", linestyle='--')
    plt.title("Precision-Recall Curve per Class")
    plt.xlabel("Recall"); plt.ylabel("Precision")
    plt.legend(); plt.grid(True)
    show_or_save()
else:
    print("[Precision-Recall curves not generated: Probability file not found.]")

# -- 7. ROC Curve per Class (if probabilities are available) --
if preds_csv_files:
    # Reuse y_true_pr_roc and y_proba from previous section
    y_test_bin = label_binarize(y_true_pr_roc, classes=list(range(n_classes)))
    plt.figure(figsize=(8,7))
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_bin[:,i], y_proba[:,i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{['No DR','Mild','Mod.','Severe','Prolif.'][i]} (AUC={roc_auc:.2f})", linestyle='-')
    plt.plot([0,1],[0,1],'k--',lw=2)
    plt.title("ROC Curve per Class")
    plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
    plt.legend(); plt.grid(True)
    show_or_save()
else:
    print("[ROC curves not generated: Probability file not found.]")

# -- 8. Final Results Table (For Paper) --
results_data = {
    'Metric':          ['Final Train Accuracy', 'Final Val Accuracy', 'Best Val Macro F1'],
    'Value':           [f"{history['train_acc'][-1]:.3f}", f"{history['val_acc'][-1]:.3f}", f"{max(history['val_macro_f1']):.3f}"],
}
results_table = pd.DataFrame(results_data)
print("\n==== FINAL SUMMARY TABLE ====\n")
display(results_table)

print("\n==== Key Results Names ====")
print(f"Training Curve: 'Train/Val Loss, Accuracy, F1-cureves (per epoch)'")
print(f"Classification Report: '{report_fp}'")
print(f"Per-class F1 Barplot: '[No DR,Mild,Moderate,Severe,Proliferative] F1-score'")
print(f"Confusion Matrix: '{cm_img_fp}'")
print(f"Confusion Matrix Heatmap: 'TAHDL Val Set Confusion'")
print(f"PR/ROC curve: '[Classwise if probs available]'", "(if probabilities were saved)") # Added clarification
print("Final Results Table is above ↑")
print("\n==== Cell 8 complete: All results ====== \n")

